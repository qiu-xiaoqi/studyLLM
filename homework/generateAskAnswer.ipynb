{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, T5ForConditionalGeneration, Text2TextGenerationPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型和tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"../uer/t5-base-chinese-cluecorpussmall\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"../uer/t5-base-chinese-cluecorpussmall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'answer', 'question', 'id'],\n",
       "         num_rows: 11616\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'answer', 'question', 'id'],\n",
       "         num_rows: 2904\n",
       "     })\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['context', 'answer', 'question', 'id'],\n",
       "     num_rows: 984\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"../data/DuReaderQG/train.json\", split=\"train\")\n",
    "datasets = dataset.train_test_split(test_size=0.2)\n",
    "dev_dataset = load_dataset(\"json\", data_files=\"../data/DuReaderQG/dev.json\", split=\"train\")\n",
    "datasets, dev_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11485, 11816,   131,   679,  5314,   872,  6432,  4415,  6389,\n",
       "           966,   749,   117,   671,  2828,  2398,  2382,   886,  4500,   117,\n",
       "          6438,  1091,  8183,   118,   100,   120,   100,   117,  3152,   816,\n",
       "          6632,  2207,  6632,  1914,   117,  6862,  2428,  6632,  2714,   100,\n",
       "          4638,  6413,  2218,  3221,   679,  1168,   122,  4907,   170,  6821,\n",
       "           702,  6656,   792,  6574,  2166,  2428,  3300,  1068,  5143,   117,\n",
       "           117,  6760,  6862,  3221,   671,   702,  2900,  3403,   117,  2166,\n",
       "          2428,  3221,  1369,   671,   702,  2900,  3403,   117,  6438,  1091,\n",
       "          6862,  2428,  2218,  3221,  1296,   855,  3198,  7313,  4828,  1928,\n",
       "          2812,  6814,  4638,  7481,  4916,   733,   809,  3144,  2945,  2166,\n",
       "          2428,   117,  4385,  1762,  1296,  4817,   122,  8165,  4638,  3683,\n",
       "          3193,  3309,  1296,  4817,   100,  4638,  2571,   679,  2208,   511,\n",
       "           671,  5663,  2769,   812,  5543,  2697,  6230,  4638,  6862,  2428,\n",
       "          2418,  6421,  1762,   100,  2340,  1381,   117,  4680,  1184,  7361,\n",
       "          1169,  1908,  1169,  4638,  6862,  2428,  4638,   712,  6206,  3221,\n",
       "          2970,  1366,   117,   100,   119,   121,  4638,  6862,  2428,  1920,\n",
       "          3519,  1762,   100,   120,   161,  2340,  1381,   511,   170,  8184,\n",
       "           131,   120,   120,  9998, 12708,  8167,   119, 11118,   119,  8134,\n",
       "           120,  9025,   136,  8654,   134,   100,   142,   100,   131, 10503,\n",
       "          8129,  3322,  3462,  4801,  4669,  6438,  1091,  6862,  2428,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_example = tokenizer(\"context: \" + datasets[\"train\"][0][\"context\"] + \"question: \" + datasets[\"train\"][0][\"question\"], return_tensors=\"pt\", max_length=256, truncation=True, padding=\"max_length\")\n",
    "tokenize_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [-100, 8183, 118, 100, 120, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = tokenizer(datasets[\"train\"][0][\"answer\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "label[\"input_ids\"][label[\"input_ids\"] == tokenizer.pad_token_id] = -100\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11485, 11816,   131,   679,  5314,   872,  6432,  4415,  6389,\n",
       "           966,   749,   117,   671,  2828,  2398,  2382,   886,  4500,   117,\n",
       "          6438,  1091,  8183,   118,   100,   120,   100,   117,  3152,   816,\n",
       "          6632,  2207,  6632,  1914,   117,  6862,  2428,  6632,  2714,   100,\n",
       "          4638,  6413,  2218,  3221,   679,  1168,   122,  4907,   170,  6821,\n",
       "           702,  6656,   792,  6574,  2166,  2428,  3300,  1068,  5143,   117,\n",
       "           117,  6760,  6862,  3221,   671,   702,  2900,  3403,   117,  2166,\n",
       "          2428,  3221,  1369,   671,   702,  2900,  3403,   117,  6438,  1091,\n",
       "          6862,  2428,  2218,  3221,  1296,   855,  3198,  7313,  4828,  1928,\n",
       "          2812,  6814,  4638,  7481,  4916,   733,   809,  3144,  2945,  2166,\n",
       "          2428,   117,  4385,  1762,  1296,  4817,   122,  8165,  4638,  3683,\n",
       "          3193,  3309,  1296,  4817,   100,  4638,  2571,   679,  2208,   511,\n",
       "           671,  5663,  2769,   812,  5543,  2697,  6230,  4638,  6862,  2428,\n",
       "          2418,  6421,  1762,   100,  2340,  1381,   117,  4680,  1184,  7361,\n",
       "          1169,  1908,  1169,  4638,  6862,  2428,  4638,   712,  6206,  3221,\n",
       "          2970,  1366,   117,   100,   119,   121,  4638,  6862,  2428,  1920,\n",
       "          3519,  1762,   100,   120,   161,  2340,  1381,   511,   170,  8184,\n",
       "           131,   120,   120,  9998, 12708,  8167,   119, 11118,   119,  8134,\n",
       "           120,  9025,   136,  8654,   134,   100,   142,   100,   131, 10503,\n",
       "          8129,  3322,  3462,  4801,  4669,  6438,  1091,  6862,  2428,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': [-100, 8183, 118, 100, 120, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_example[\"labels\"] = label[\"input_ids\"]\n",
    "tokenize_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11616\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2904\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_function(examples):\n",
    "    context_questions = [f\"context:{context} question:{question}\" for context, question in zip(examples[\"context\"], examples[\"question\"])]\n",
    "    tokenized_examples = tokenizer(context_questions, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(examples[\"answer\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    labels[\"input_ids\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    tokenized_examples[\"labels\"] = labels[\"input_ids\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  11485,\n",
       "  11816,\n",
       "  131,\n",
       "  679,\n",
       "  5314,\n",
       "  872,\n",
       "  6432,\n",
       "  4415,\n",
       "  6389,\n",
       "  966,\n",
       "  749,\n",
       "  117,\n",
       "  671,\n",
       "  2828,\n",
       "  2398,\n",
       "  2382,\n",
       "  886,\n",
       "  4500,\n",
       "  117,\n",
       "  6438,\n",
       "  1091,\n",
       "  8183,\n",
       "  118,\n",
       "  100,\n",
       "  120,\n",
       "  100,\n",
       "  117,\n",
       "  3152,\n",
       "  816,\n",
       "  6632,\n",
       "  2207,\n",
       "  6632,\n",
       "  1914,\n",
       "  117,\n",
       "  6862,\n",
       "  2428,\n",
       "  6632,\n",
       "  2714,\n",
       "  100,\n",
       "  4638,\n",
       "  6413,\n",
       "  2218,\n",
       "  3221,\n",
       "  679,\n",
       "  1168,\n",
       "  122,\n",
       "  4907,\n",
       "  170,\n",
       "  6821,\n",
       "  702,\n",
       "  6656,\n",
       "  792,\n",
       "  6574,\n",
       "  2166,\n",
       "  2428,\n",
       "  3300,\n",
       "  1068,\n",
       "  5143,\n",
       "  117,\n",
       "  117,\n",
       "  6760,\n",
       "  6862,\n",
       "  3221,\n",
       "  671,\n",
       "  702,\n",
       "  2900,\n",
       "  3403,\n",
       "  117,\n",
       "  2166,\n",
       "  2428,\n",
       "  3221,\n",
       "  1369,\n",
       "  671,\n",
       "  702,\n",
       "  2900,\n",
       "  3403,\n",
       "  117,\n",
       "  6438,\n",
       "  1091,\n",
       "  6862,\n",
       "  2428,\n",
       "  2218,\n",
       "  3221,\n",
       "  1296,\n",
       "  855,\n",
       "  3198,\n",
       "  7313,\n",
       "  4828,\n",
       "  1928,\n",
       "  2812,\n",
       "  6814,\n",
       "  4638,\n",
       "  7481,\n",
       "  4916,\n",
       "  733,\n",
       "  809,\n",
       "  3144,\n",
       "  2945,\n",
       "  2166,\n",
       "  2428,\n",
       "  117,\n",
       "  4385,\n",
       "  1762,\n",
       "  1296,\n",
       "  4817,\n",
       "  122,\n",
       "  8165,\n",
       "  4638,\n",
       "  3683,\n",
       "  3193,\n",
       "  3309,\n",
       "  1296,\n",
       "  4817,\n",
       "  100,\n",
       "  4638,\n",
       "  2571,\n",
       "  679,\n",
       "  2208,\n",
       "  511,\n",
       "  671,\n",
       "  5663,\n",
       "  2769,\n",
       "  812,\n",
       "  5543,\n",
       "  2697,\n",
       "  6230,\n",
       "  4638,\n",
       "  6862,\n",
       "  2428,\n",
       "  2418,\n",
       "  6421,\n",
       "  1762,\n",
       "  100,\n",
       "  2340,\n",
       "  1381,\n",
       "  117,\n",
       "  4680,\n",
       "  1184,\n",
       "  7361,\n",
       "  1169,\n",
       "  1908,\n",
       "  1169,\n",
       "  4638,\n",
       "  6862,\n",
       "  2428,\n",
       "  4638,\n",
       "  712,\n",
       "  6206,\n",
       "  3221,\n",
       "  2970,\n",
       "  1366,\n",
       "  117,\n",
       "  100,\n",
       "  119,\n",
       "  121,\n",
       "  4638,\n",
       "  6862,\n",
       "  2428,\n",
       "  1920,\n",
       "  3519,\n",
       "  1762,\n",
       "  100,\n",
       "  120,\n",
       "  161,\n",
       "  2340,\n",
       "  1381,\n",
       "  511,\n",
       "  170,\n",
       "  8184,\n",
       "  131,\n",
       "  120,\n",
       "  120,\n",
       "  9998,\n",
       "  12708,\n",
       "  8167,\n",
       "  119,\n",
       "  11118,\n",
       "  119,\n",
       "  8134,\n",
       "  120,\n",
       "  9025,\n",
       "  136,\n",
       "  8654,\n",
       "  134,\n",
       "  100,\n",
       "  142,\n",
       "  100,\n",
       "  159,\n",
       "  8803,\n",
       "  8415,\n",
       "  8410,\n",
       "  131,\n",
       "  10503,\n",
       "  8129,\n",
       "  3322,\n",
       "  3462,\n",
       "  4801,\n",
       "  4669,\n",
       "  6438,\n",
       "  1091,\n",
       "  6862,\n",
       "  2428,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [101,\n",
       "  8183,\n",
       "  118,\n",
       "  100,\n",
       "  120,\n",
       "  100,\n",
       "  102,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./output/generateAskAnswer\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DefaultDataCollator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6f66a2ecb7441abe735e8a663702c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.1569, 'learning_rate': 9.986225895316806e-05, 'epoch': 0.03}\n",
      "{'loss': 8.7968, 'learning_rate': 9.972451790633609e-05, 'epoch': 0.06}\n",
      "{'loss': 8.4562, 'learning_rate': 9.958677685950414e-05, 'epoch': 0.08}\n",
      "{'loss': 8.2263, 'learning_rate': 9.944903581267218e-05, 'epoch': 0.11}\n",
      "{'loss': 8.0821, 'learning_rate': 9.931129476584022e-05, 'epoch': 0.14}\n",
      "{'loss': 7.9258, 'learning_rate': 9.917355371900827e-05, 'epoch': 0.17}\n",
      "{'loss': 7.7649, 'learning_rate': 9.903581267217631e-05, 'epoch': 0.19}\n",
      "{'loss': 7.6729, 'learning_rate': 9.889807162534436e-05, 'epoch': 0.22}\n",
      "{'loss': 7.5566, 'learning_rate': 9.876033057851241e-05, 'epoch': 0.25}\n",
      "{'loss': 7.4418, 'learning_rate': 9.862258953168044e-05, 'epoch': 0.28}\n",
      "{'loss': 7.3313, 'learning_rate': 9.848484848484849e-05, 'epoch': 0.3}\n",
      "{'loss': 7.2631, 'learning_rate': 9.834710743801654e-05, 'epoch': 0.33}\n",
      "{'loss': 7.1633, 'learning_rate': 9.820936639118457e-05, 'epoch': 0.36}\n",
      "{'loss': 7.0557, 'learning_rate': 9.807162534435263e-05, 'epoch': 0.39}\n",
      "{'loss': 7.0175, 'learning_rate': 9.793388429752067e-05, 'epoch': 0.41}\n",
      "{'loss': 6.8732, 'learning_rate': 9.77961432506887e-05, 'epoch': 0.44}\n",
      "{'loss': 6.8463, 'learning_rate': 9.765840220385676e-05, 'epoch': 0.47}\n",
      "{'loss': 6.8379, 'learning_rate': 9.75206611570248e-05, 'epoch': 0.5}\n",
      "{'loss': 6.7136, 'learning_rate': 9.738292011019284e-05, 'epoch': 0.52}\n",
      "{'loss': 6.6776, 'learning_rate': 9.72451790633609e-05, 'epoch': 0.55}\n",
      "{'loss': 6.6354, 'learning_rate': 9.710743801652894e-05, 'epoch': 0.58}\n",
      "{'loss': 6.659, 'learning_rate': 9.696969696969698e-05, 'epoch': 0.61}\n",
      "{'loss': 6.5421, 'learning_rate': 9.683195592286502e-05, 'epoch': 0.63}\n",
      "{'loss': 6.4893, 'learning_rate': 9.669421487603306e-05, 'epoch': 0.66}\n",
      "{'loss': 6.448, 'learning_rate': 9.655647382920111e-05, 'epoch': 0.69}\n",
      "{'loss': 6.4403, 'learning_rate': 9.641873278236915e-05, 'epoch': 0.72}\n",
      "{'loss': 6.3819, 'learning_rate': 9.628099173553719e-05, 'epoch': 0.74}\n",
      "{'loss': 6.3691, 'learning_rate': 9.614325068870525e-05, 'epoch': 0.77}\n",
      "{'loss': 6.3457, 'learning_rate': 9.600550964187329e-05, 'epoch': 0.8}\n",
      "{'loss': 6.2835, 'learning_rate': 9.586776859504133e-05, 'epoch': 0.83}\n",
      "{'loss': 6.2231, 'learning_rate': 9.573002754820937e-05, 'epoch': 0.85}\n",
      "{'loss': 6.21, 'learning_rate': 9.559228650137742e-05, 'epoch': 0.88}\n",
      "{'loss': 6.157, 'learning_rate': 9.545454545454546e-05, 'epoch': 0.91}\n",
      "{'loss': 6.1529, 'learning_rate': 9.53168044077135e-05, 'epoch': 0.94}\n",
      "{'loss': 6.1059, 'learning_rate': 9.517906336088154e-05, 'epoch': 0.96}\n",
      "{'loss': 6.0436, 'learning_rate': 9.50413223140496e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b6e906498c4cfe8ab4df1c4ac77fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.653988838195801, 'eval_runtime': 96.0189, 'eval_samples_per_second': 30.244, 'eval_steps_per_second': 3.781, 'epoch': 1.0}\n",
      "{'loss': 6.0263, 'learning_rate': 9.490358126721764e-05, 'epoch': 1.02}\n",
      "{'loss': 5.9873, 'learning_rate': 9.476584022038568e-05, 'epoch': 1.05}\n",
      "{'loss': 6.0252, 'learning_rate': 9.462809917355372e-05, 'epoch': 1.07}\n",
      "{'loss': 5.9483, 'learning_rate': 9.449035812672177e-05, 'epoch': 1.1}\n",
      "{'loss': 5.9569, 'learning_rate': 9.435261707988981e-05, 'epoch': 1.13}\n",
      "{'loss': 5.9447, 'learning_rate': 9.421487603305785e-05, 'epoch': 1.16}\n",
      "{'loss': 5.8284, 'learning_rate': 9.40771349862259e-05, 'epoch': 1.18}\n",
      "{'loss': 5.8181, 'learning_rate': 9.393939393939395e-05, 'epoch': 1.21}\n",
      "{'loss': 5.8048, 'learning_rate': 9.380165289256199e-05, 'epoch': 1.24}\n",
      "{'loss': 5.8205, 'learning_rate': 9.366391184573004e-05, 'epoch': 1.27}\n",
      "{'loss': 5.7426, 'learning_rate': 9.352617079889807e-05, 'epoch': 1.29}\n",
      "{'loss': 5.74, 'learning_rate': 9.338842975206612e-05, 'epoch': 1.32}\n",
      "{'loss': 5.7433, 'learning_rate': 9.325068870523416e-05, 'epoch': 1.35}\n",
      "{'loss': 5.7553, 'learning_rate': 9.31129476584022e-05, 'epoch': 1.38}\n",
      "{'loss': 5.7319, 'learning_rate': 9.297520661157026e-05, 'epoch': 1.4}\n",
      "{'loss': 5.6532, 'learning_rate': 9.28374655647383e-05, 'epoch': 1.43}\n",
      "{'loss': 5.6052, 'learning_rate': 9.269972451790634e-05, 'epoch': 1.46}\n",
      "{'loss': 5.6585, 'learning_rate': 9.256198347107439e-05, 'epoch': 1.49}\n",
      "{'loss': 5.5896, 'learning_rate': 9.242424242424242e-05, 'epoch': 1.52}\n",
      "{'loss': 5.5402, 'learning_rate': 9.228650137741047e-05, 'epoch': 1.54}\n",
      "{'loss': 5.5381, 'learning_rate': 9.214876033057853e-05, 'epoch': 1.57}\n",
      "{'loss': 5.5599, 'learning_rate': 9.201101928374655e-05, 'epoch': 1.6}\n",
      "{'loss': 5.536, 'learning_rate': 9.187327823691461e-05, 'epoch': 1.63}\n",
      "{'loss': 5.5573, 'learning_rate': 9.173553719008266e-05, 'epoch': 1.65}\n",
      "{'loss': 5.4913, 'learning_rate': 9.159779614325069e-05, 'epoch': 1.68}\n",
      "{'loss': 5.4334, 'learning_rate': 9.146005509641874e-05, 'epoch': 1.71}\n",
      "{'loss': 5.3988, 'learning_rate': 9.132231404958678e-05, 'epoch': 1.74}\n",
      "{'loss': 5.3953, 'learning_rate': 9.118457300275482e-05, 'epoch': 1.76}\n",
      "{'loss': 5.3334, 'learning_rate': 9.104683195592288e-05, 'epoch': 1.79}\n",
      "{'loss': 5.4084, 'learning_rate': 9.090909090909092e-05, 'epoch': 1.82}\n",
      "{'loss': 5.383, 'learning_rate': 9.077134986225896e-05, 'epoch': 1.85}\n",
      "{'loss': 5.2913, 'learning_rate': 9.063360881542701e-05, 'epoch': 1.87}\n",
      "{'loss': 5.3734, 'learning_rate': 9.049586776859504e-05, 'epoch': 1.9}\n",
      "{'loss': 5.3155, 'learning_rate': 9.035812672176309e-05, 'epoch': 1.93}\n",
      "{'loss': 5.246, 'learning_rate': 9.022038567493113e-05, 'epoch': 1.96}\n",
      "{'loss': 5.2566, 'learning_rate': 9.008264462809917e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036c8ec75abe4bc2b3a6edae47f2c67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.818799018859863, 'eval_runtime': 95.9488, 'eval_samples_per_second': 30.266, 'eval_steps_per_second': 3.783, 'epoch': 2.0}\n",
      "{'loss': 5.1992, 'learning_rate': 8.994490358126723e-05, 'epoch': 2.01}\n",
      "{'loss': 5.0955, 'learning_rate': 8.980716253443527e-05, 'epoch': 2.04}\n",
      "{'loss': 5.1757, 'learning_rate': 8.966942148760331e-05, 'epoch': 2.07}\n",
      "{'loss': 5.197, 'learning_rate': 8.953168044077136e-05, 'epoch': 2.09}\n",
      "{'loss': 5.2297, 'learning_rate': 8.93939393939394e-05, 'epoch': 2.12}\n",
      "{'loss': 5.1034, 'learning_rate': 8.925619834710744e-05, 'epoch': 2.15}\n",
      "{'loss': 5.1135, 'learning_rate': 8.911845730027548e-05, 'epoch': 2.18}\n",
      "{'loss': 5.087, 'learning_rate': 8.898071625344352e-05, 'epoch': 2.2}\n",
      "{'loss': 5.0825, 'learning_rate': 8.884297520661158e-05, 'epoch': 2.23}\n",
      "{'loss': 5.109, 'learning_rate': 8.870523415977962e-05, 'epoch': 2.26}\n",
      "{'loss': 5.0172, 'learning_rate': 8.856749311294766e-05, 'epoch': 2.29}\n",
      "{'loss': 5.0702, 'learning_rate': 8.842975206611571e-05, 'epoch': 2.31}\n",
      "{'loss': 5.0175, 'learning_rate': 8.829201101928375e-05, 'epoch': 2.34}\n",
      "{'loss': 5.0111, 'learning_rate': 8.81542699724518e-05, 'epoch': 2.37}\n",
      "{'loss': 4.9837, 'learning_rate': 8.801652892561983e-05, 'epoch': 2.4}\n",
      "{'loss': 5.0355, 'learning_rate': 8.787878787878789e-05, 'epoch': 2.42}\n",
      "{'loss': 4.9514, 'learning_rate': 8.774104683195593e-05, 'epoch': 2.45}\n",
      "{'loss': 4.9065, 'learning_rate': 8.760330578512397e-05, 'epoch': 2.48}\n",
      "{'loss': 4.974, 'learning_rate': 8.746556473829202e-05, 'epoch': 2.51}\n",
      "{'loss': 4.838, 'learning_rate': 8.732782369146006e-05, 'epoch': 2.53}\n",
      "{'loss': 4.8785, 'learning_rate': 8.71900826446281e-05, 'epoch': 2.56}\n",
      "{'loss': 4.8833, 'learning_rate': 8.705234159779614e-05, 'epoch': 2.59}\n",
      "{'loss': 4.9227, 'learning_rate': 8.691460055096418e-05, 'epoch': 2.62}\n",
      "{'loss': 4.8362, 'learning_rate': 8.677685950413224e-05, 'epoch': 2.64}\n",
      "{'loss': 4.7622, 'learning_rate': 8.663911845730028e-05, 'epoch': 2.67}\n",
      "{'loss': 4.8536, 'learning_rate': 8.650137741046832e-05, 'epoch': 2.7}\n",
      "{'loss': 4.8039, 'learning_rate': 8.636363636363637e-05, 'epoch': 2.73}\n",
      "{'loss': 4.8925, 'learning_rate': 8.622589531680441e-05, 'epoch': 2.75}\n",
      "{'loss': 4.8152, 'learning_rate': 8.608815426997245e-05, 'epoch': 2.78}\n",
      "{'loss': 4.847, 'learning_rate': 8.595041322314051e-05, 'epoch': 2.81}\n",
      "{'loss': 4.7309, 'learning_rate': 8.581267217630854e-05, 'epoch': 2.84}\n",
      "{'loss': 4.7734, 'learning_rate': 8.567493112947659e-05, 'epoch': 2.87}\n",
      "{'loss': 4.7375, 'learning_rate': 8.553719008264463e-05, 'epoch': 2.89}\n",
      "{'loss': 4.6798, 'learning_rate': 8.539944903581267e-05, 'epoch': 2.92}\n",
      "{'loss': 4.7124, 'learning_rate': 8.526170798898072e-05, 'epoch': 2.95}\n",
      "{'loss': 4.6937, 'learning_rate': 8.512396694214876e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abda1ea477f74e2da0cabf2059d48563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.2185258865356445, 'eval_runtime': 96.1917, 'eval_samples_per_second': 30.19, 'eval_steps_per_second': 3.774, 'epoch': 3.0}\n",
      "{'loss': 4.769, 'learning_rate': 8.49862258953168e-05, 'epoch': 3.0}\n",
      "{'loss': 4.5634, 'learning_rate': 8.484848484848486e-05, 'epoch': 3.03}\n",
      "{'loss': 4.6664, 'learning_rate': 8.471074380165289e-05, 'epoch': 3.06}\n",
      "{'loss': 4.6635, 'learning_rate': 8.457300275482094e-05, 'epoch': 3.09}\n",
      "{'loss': 4.611, 'learning_rate': 8.4435261707989e-05, 'epoch': 3.11}\n",
      "{'loss': 4.5173, 'learning_rate': 8.429752066115702e-05, 'epoch': 3.14}\n",
      "{'loss': 4.542, 'learning_rate': 8.415977961432507e-05, 'epoch': 3.17}\n",
      "{'loss': 4.6052, 'learning_rate': 8.402203856749313e-05, 'epoch': 3.2}\n",
      "{'loss': 4.5743, 'learning_rate': 8.388429752066116e-05, 'epoch': 3.22}\n",
      "{'loss': 4.5031, 'learning_rate': 8.374655647382921e-05, 'epoch': 3.25}\n",
      "{'loss': 4.4975, 'learning_rate': 8.360881542699725e-05, 'epoch': 3.28}\n",
      "{'loss': 4.4079, 'learning_rate': 8.347107438016529e-05, 'epoch': 3.31}\n",
      "{'loss': 4.5143, 'learning_rate': 8.333333333333334e-05, 'epoch': 3.33}\n",
      "{'loss': 4.5008, 'learning_rate': 8.319559228650138e-05, 'epoch': 3.36}\n",
      "{'loss': 4.4756, 'learning_rate': 8.305785123966942e-05, 'epoch': 3.39}\n",
      "{'loss': 4.3919, 'learning_rate': 8.292011019283748e-05, 'epoch': 3.42}\n",
      "{'loss': 4.3437, 'learning_rate': 8.27823691460055e-05, 'epoch': 3.44}\n",
      "{'loss': 4.4508, 'learning_rate': 8.264462809917356e-05, 'epoch': 3.47}\n",
      "{'loss': 4.3718, 'learning_rate': 8.25068870523416e-05, 'epoch': 3.5}\n",
      "{'loss': 4.4064, 'learning_rate': 8.236914600550964e-05, 'epoch': 3.53}\n",
      "{'loss': 4.3698, 'learning_rate': 8.22314049586777e-05, 'epoch': 3.55}\n",
      "{'loss': 4.291, 'learning_rate': 8.209366391184574e-05, 'epoch': 3.58}\n",
      "{'loss': 4.3431, 'learning_rate': 8.195592286501378e-05, 'epoch': 3.61}\n",
      "{'loss': 4.2797, 'learning_rate': 8.181818181818183e-05, 'epoch': 3.64}\n",
      "{'loss': 4.312, 'learning_rate': 8.168044077134987e-05, 'epoch': 3.66}\n",
      "{'loss': 4.3041, 'learning_rate': 8.154269972451791e-05, 'epoch': 3.69}\n",
      "{'loss': 4.2474, 'learning_rate': 8.140495867768595e-05, 'epoch': 3.72}\n",
      "{'loss': 4.2043, 'learning_rate': 8.126721763085399e-05, 'epoch': 3.75}\n",
      "{'loss': 4.2143, 'learning_rate': 8.112947658402205e-05, 'epoch': 3.77}\n",
      "{'loss': 4.2928, 'learning_rate': 8.099173553719009e-05, 'epoch': 3.8}\n",
      "{'loss': 4.2124, 'learning_rate': 8.085399449035813e-05, 'epoch': 3.83}\n",
      "{'loss': 4.1338, 'learning_rate': 8.071625344352618e-05, 'epoch': 3.86}\n",
      "{'loss': 4.1855, 'learning_rate': 8.057851239669422e-05, 'epoch': 3.88}\n",
      "{'loss': 4.2148, 'learning_rate': 8.044077134986226e-05, 'epoch': 3.91}\n",
      "{'loss': 4.1503, 'learning_rate': 8.03030303030303e-05, 'epoch': 3.94}\n",
      "{'loss': 4.1651, 'learning_rate': 8.016528925619836e-05, 'epoch': 3.97}\n",
      "{'loss': 4.1465, 'learning_rate': 8.00275482093664e-05, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d6c9ac58d34ed2ab2e347fd34d32b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7030255794525146, 'eval_runtime': 96.0142, 'eval_samples_per_second': 30.246, 'eval_steps_per_second': 3.781, 'epoch': 4.0}\n",
      "{'loss': 4.1271, 'learning_rate': 7.988980716253444e-05, 'epoch': 4.02}\n",
      "{'loss': 4.0743, 'learning_rate': 7.975206611570249e-05, 'epoch': 4.05}\n",
      "{'loss': 4.0768, 'learning_rate': 7.961432506887053e-05, 'epoch': 4.08}\n",
      "{'loss': 4.0383, 'learning_rate': 7.947658402203857e-05, 'epoch': 4.1}\n",
      "{'loss': 4.0793, 'learning_rate': 7.933884297520661e-05, 'epoch': 4.13}\n",
      "{'loss': 4.0928, 'learning_rate': 7.920110192837465e-05, 'epoch': 4.16}\n",
      "{'loss': 3.9914, 'learning_rate': 7.90633608815427e-05, 'epoch': 4.19}\n",
      "{'loss': 4.0341, 'learning_rate': 7.892561983471075e-05, 'epoch': 4.21}\n",
      "{'loss': 3.9609, 'learning_rate': 7.878787878787879e-05, 'epoch': 4.24}\n",
      "{'loss': 3.9648, 'learning_rate': 7.865013774104684e-05, 'epoch': 4.27}\n",
      "{'loss': 3.9888, 'learning_rate': 7.851239669421488e-05, 'epoch': 4.3}\n",
      "{'loss': 3.9157, 'learning_rate': 7.837465564738292e-05, 'epoch': 4.33}\n",
      "{'loss': 3.9239, 'learning_rate': 7.823691460055098e-05, 'epoch': 4.35}\n",
      "{'loss': 3.8255, 'learning_rate': 7.8099173553719e-05, 'epoch': 4.38}\n",
      "{'loss': 3.9284, 'learning_rate': 7.796143250688706e-05, 'epoch': 4.41}\n",
      "{'loss': 3.9474, 'learning_rate': 7.782369146005511e-05, 'epoch': 4.44}\n",
      "{'loss': 3.8644, 'learning_rate': 7.768595041322314e-05, 'epoch': 4.46}\n",
      "{'loss': 3.8189, 'learning_rate': 7.754820936639119e-05, 'epoch': 4.49}\n",
      "{'loss': 3.7935, 'learning_rate': 7.741046831955923e-05, 'epoch': 4.52}\n",
      "{'loss': 3.8346, 'learning_rate': 7.727272727272727e-05, 'epoch': 4.55}\n",
      "{'loss': 3.7995, 'learning_rate': 7.713498622589533e-05, 'epoch': 4.57}\n",
      "{'loss': 3.7149, 'learning_rate': 7.699724517906337e-05, 'epoch': 4.6}\n",
      "{'loss': 3.7655, 'learning_rate': 7.685950413223141e-05, 'epoch': 4.63}\n",
      "{'loss': 3.7444, 'learning_rate': 7.672176308539946e-05, 'epoch': 4.66}\n",
      "{'loss': 3.7194, 'learning_rate': 7.658402203856749e-05, 'epoch': 4.68}\n",
      "{'loss': 3.6864, 'learning_rate': 7.644628099173554e-05, 'epoch': 4.71}\n",
      "{'loss': 3.7642, 'learning_rate': 7.63085399449036e-05, 'epoch': 4.74}\n",
      "{'loss': 3.4937, 'learning_rate': 7.617079889807162e-05, 'epoch': 4.77}\n",
      "{'loss': 3.6243, 'learning_rate': 7.603305785123968e-05, 'epoch': 4.79}\n",
      "{'loss': 3.5765, 'learning_rate': 7.589531680440772e-05, 'epoch': 4.82}\n",
      "{'loss': 3.618, 'learning_rate': 7.575757575757576e-05, 'epoch': 4.85}\n",
      "{'loss': 3.5526, 'learning_rate': 7.561983471074381e-05, 'epoch': 4.88}\n",
      "{'loss': 3.5225, 'learning_rate': 7.548209366391185e-05, 'epoch': 4.9}\n",
      "{'loss': 3.5522, 'learning_rate': 7.534435261707989e-05, 'epoch': 4.93}\n",
      "{'loss': 3.5437, 'learning_rate': 7.520661157024795e-05, 'epoch': 4.96}\n",
      "{'loss': 3.4411, 'learning_rate': 7.506887052341597e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93f98733dd847508d1ccfd013ebbf39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.116034746170044, 'eval_runtime': 96.2021, 'eval_samples_per_second': 30.186, 'eval_steps_per_second': 3.773, 'epoch': 5.0}\n",
      "{'loss': 3.5138, 'learning_rate': 7.493112947658403e-05, 'epoch': 5.01}\n",
      "{'loss': 3.4943, 'learning_rate': 7.479338842975207e-05, 'epoch': 5.04}\n",
      "{'loss': 3.5393, 'learning_rate': 7.465564738292011e-05, 'epoch': 5.07}\n",
      "{'loss': 3.4522, 'learning_rate': 7.451790633608816e-05, 'epoch': 5.1}\n",
      "{'loss': 3.4014, 'learning_rate': 7.43801652892562e-05, 'epoch': 5.12}\n",
      "{'loss': 3.4114, 'learning_rate': 7.424242424242424e-05, 'epoch': 5.15}\n",
      "{'loss': 3.3911, 'learning_rate': 7.41046831955923e-05, 'epoch': 5.18}\n",
      "{'loss': 3.3525, 'learning_rate': 7.396694214876034e-05, 'epoch': 5.21}\n",
      "{'loss': 3.231, 'learning_rate': 7.382920110192838e-05, 'epoch': 5.23}\n",
      "{'loss': 3.2769, 'learning_rate': 7.369146005509642e-05, 'epoch': 5.26}\n",
      "{'loss': 3.265, 'learning_rate': 7.355371900826447e-05, 'epoch': 5.29}\n",
      "{'loss': 3.2153, 'learning_rate': 7.341597796143251e-05, 'epoch': 5.32}\n",
      "{'loss': 3.1585, 'learning_rate': 7.327823691460055e-05, 'epoch': 5.34}\n",
      "{'loss': 3.0524, 'learning_rate': 7.31404958677686e-05, 'epoch': 5.37}\n",
      "{'loss': 3.1188, 'learning_rate': 7.300275482093665e-05, 'epoch': 5.4}\n",
      "{'loss': 3.0888, 'learning_rate': 7.286501377410469e-05, 'epoch': 5.43}\n",
      "{'loss': 3.1354, 'learning_rate': 7.272727272727273e-05, 'epoch': 5.45}\n",
      "{'loss': 3.0271, 'learning_rate': 7.258953168044077e-05, 'epoch': 5.48}\n",
      "{'loss': 3.1389, 'learning_rate': 7.245179063360882e-05, 'epoch': 5.51}\n",
      "{'loss': 3.0799, 'learning_rate': 7.231404958677686e-05, 'epoch': 5.54}\n",
      "{'loss': 3.0331, 'learning_rate': 7.21763085399449e-05, 'epoch': 5.56}\n",
      "{'loss': 3.0474, 'learning_rate': 7.203856749311296e-05, 'epoch': 5.59}\n",
      "{'loss': 3.0314, 'learning_rate': 7.1900826446281e-05, 'epoch': 5.62}\n",
      "{'loss': 3.0671, 'learning_rate': 7.176308539944904e-05, 'epoch': 5.65}\n",
      "{'loss': 2.971, 'learning_rate': 7.162534435261708e-05, 'epoch': 5.67}\n",
      "{'loss': 3.0168, 'learning_rate': 7.148760330578512e-05, 'epoch': 5.7}\n",
      "{'loss': 3.0695, 'learning_rate': 7.134986225895317e-05, 'epoch': 5.73}\n",
      "{'loss': 3.0246, 'learning_rate': 7.121212121212121e-05, 'epoch': 5.76}\n",
      "{'loss': 2.9405, 'learning_rate': 7.107438016528925e-05, 'epoch': 5.79}\n",
      "{'loss': 2.9618, 'learning_rate': 7.093663911845731e-05, 'epoch': 5.81}\n",
      "{'loss': 2.9326, 'learning_rate': 7.079889807162535e-05, 'epoch': 5.84}\n",
      "{'loss': 3.014, 'learning_rate': 7.066115702479339e-05, 'epoch': 5.87}\n",
      "{'loss': 2.9408, 'learning_rate': 7.052341597796144e-05, 'epoch': 5.9}\n",
      "{'loss': 2.9846, 'learning_rate': 7.038567493112947e-05, 'epoch': 5.92}\n",
      "{'loss': 2.8707, 'learning_rate': 7.024793388429752e-05, 'epoch': 5.95}\n",
      "{'loss': 2.8227, 'learning_rate': 7.011019283746558e-05, 'epoch': 5.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df53d24b387542968c82e0abb27fdd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4440579414367676, 'eval_runtime': 96.2043, 'eval_samples_per_second': 30.186, 'eval_steps_per_second': 3.773, 'epoch': 6.0}\n",
      "{'loss': 2.9554, 'learning_rate': 6.99724517906336e-05, 'epoch': 6.01}\n",
      "{'loss': 2.7866, 'learning_rate': 6.983471074380166e-05, 'epoch': 6.03}\n",
      "{'loss': 2.9387, 'learning_rate': 6.96969696969697e-05, 'epoch': 6.06}\n",
      "{'loss': 2.8636, 'learning_rate': 6.955922865013774e-05, 'epoch': 6.09}\n",
      "{'loss': 2.7669, 'learning_rate': 6.94214876033058e-05, 'epoch': 6.12}\n",
      "{'loss': 2.7974, 'learning_rate': 6.928374655647383e-05, 'epoch': 6.14}\n",
      "{'loss': 2.7611, 'learning_rate': 6.914600550964187e-05, 'epoch': 6.17}\n",
      "{'loss': 2.8568, 'learning_rate': 6.900826446280993e-05, 'epoch': 6.2}\n",
      "{'loss': 2.8, 'learning_rate': 6.887052341597796e-05, 'epoch': 6.23}\n",
      "{'loss': 2.748, 'learning_rate': 6.873278236914601e-05, 'epoch': 6.25}\n",
      "{'loss': 2.7573, 'learning_rate': 6.859504132231406e-05, 'epoch': 6.28}\n",
      "{'loss': 2.7592, 'learning_rate': 6.845730027548209e-05, 'epoch': 6.31}\n",
      "{'loss': 2.818, 'learning_rate': 6.831955922865014e-05, 'epoch': 6.34}\n",
      "{'loss': 2.7635, 'learning_rate': 6.818181818181818e-05, 'epoch': 6.36}\n",
      "{'loss': 2.8148, 'learning_rate': 6.804407713498622e-05, 'epoch': 6.39}\n",
      "{'loss': 2.7372, 'learning_rate': 6.790633608815428e-05, 'epoch': 6.42}\n",
      "{'loss': 2.6887, 'learning_rate': 6.776859504132232e-05, 'epoch': 6.45}\n",
      "{'loss': 2.6987, 'learning_rate': 6.763085399449036e-05, 'epoch': 6.47}\n",
      "{'loss': 2.6953, 'learning_rate': 6.749311294765841e-05, 'epoch': 6.5}\n",
      "{'loss': 2.6642, 'learning_rate': 6.735537190082644e-05, 'epoch': 6.53}\n",
      "{'loss': 2.707, 'learning_rate': 6.72176308539945e-05, 'epoch': 6.56}\n",
      "{'loss': 2.7938, 'learning_rate': 6.707988980716254e-05, 'epoch': 6.58}\n",
      "{'loss': 2.6564, 'learning_rate': 6.694214876033058e-05, 'epoch': 6.61}\n",
      "{'loss': 2.6253, 'learning_rate': 6.680440771349863e-05, 'epoch': 6.64}\n",
      "{'loss': 2.6557, 'learning_rate': 6.666666666666667e-05, 'epoch': 6.67}\n",
      "{'loss': 2.6514, 'learning_rate': 6.652892561983471e-05, 'epoch': 6.69}\n",
      "{'loss': 2.6494, 'learning_rate': 6.639118457300276e-05, 'epoch': 6.72}\n",
      "{'loss': 2.5979, 'learning_rate': 6.62534435261708e-05, 'epoch': 6.75}\n",
      "{'loss': 2.5424, 'learning_rate': 6.611570247933885e-05, 'epoch': 6.78}\n",
      "{'loss': 2.5593, 'learning_rate': 6.597796143250689e-05, 'epoch': 6.8}\n",
      "{'loss': 2.5692, 'learning_rate': 6.584022038567494e-05, 'epoch': 6.83}\n",
      "{'loss': 2.6987, 'learning_rate': 6.570247933884298e-05, 'epoch': 6.86}\n",
      "{'loss': 2.5712, 'learning_rate': 6.556473829201102e-05, 'epoch': 6.89}\n",
      "{'loss': 2.6218, 'learning_rate': 6.542699724517906e-05, 'epoch': 6.91}\n",
      "{'loss': 2.5684, 'learning_rate': 6.528925619834711e-05, 'epoch': 6.94}\n",
      "{'loss': 2.5636, 'learning_rate': 6.515151515151516e-05, 'epoch': 6.97}\n",
      "{'loss': 2.661, 'learning_rate': 6.50137741046832e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377a2c3636a44ca0964689dfca8e1ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1205883026123047, 'eval_runtime': 96.2551, 'eval_samples_per_second': 30.17, 'eval_steps_per_second': 3.771, 'epoch': 7.0}\n",
      "{'loss': 2.588, 'learning_rate': 6.487603305785124e-05, 'epoch': 7.02}\n",
      "{'loss': 2.5216, 'learning_rate': 6.473829201101929e-05, 'epoch': 7.05}\n",
      "{'loss': 2.5415, 'learning_rate': 6.460055096418733e-05, 'epoch': 7.08}\n",
      "{'loss': 2.5201, 'learning_rate': 6.446280991735537e-05, 'epoch': 7.11}\n",
      "{'loss': 2.4466, 'learning_rate': 6.432506887052342e-05, 'epoch': 7.13}\n",
      "{'loss': 2.4133, 'learning_rate': 6.418732782369147e-05, 'epoch': 7.16}\n",
      "{'loss': 2.5118, 'learning_rate': 6.40495867768595e-05, 'epoch': 7.19}\n",
      "{'loss': 2.5842, 'learning_rate': 6.391184573002756e-05, 'epoch': 7.22}\n",
      "{'loss': 2.4682, 'learning_rate': 6.377410468319559e-05, 'epoch': 7.25}\n",
      "{'loss': 2.4768, 'learning_rate': 6.363636363636364e-05, 'epoch': 7.27}\n",
      "{'loss': 2.6367, 'learning_rate': 6.349862258953168e-05, 'epoch': 7.3}\n",
      "{'loss': 2.5294, 'learning_rate': 6.336088154269972e-05, 'epoch': 7.33}\n",
      "{'loss': 2.5311, 'learning_rate': 6.322314049586778e-05, 'epoch': 7.36}\n",
      "{'loss': 2.4114, 'learning_rate': 6.308539944903582e-05, 'epoch': 7.38}\n",
      "{'loss': 2.4694, 'learning_rate': 6.294765840220386e-05, 'epoch': 7.41}\n",
      "{'loss': 2.4309, 'learning_rate': 6.280991735537191e-05, 'epoch': 7.44}\n",
      "{'loss': 2.5351, 'learning_rate': 6.267217630853994e-05, 'epoch': 7.47}\n",
      "{'loss': 2.3828, 'learning_rate': 6.253443526170799e-05, 'epoch': 7.49}\n",
      "{'loss': 2.461, 'learning_rate': 6.239669421487605e-05, 'epoch': 7.52}\n",
      "{'loss': 2.4201, 'learning_rate': 6.225895316804407e-05, 'epoch': 7.55}\n",
      "{'loss': 2.5407, 'learning_rate': 6.212121212121213e-05, 'epoch': 7.58}\n",
      "{'loss': 2.4257, 'learning_rate': 6.198347107438017e-05, 'epoch': 7.6}\n",
      "{'loss': 2.3302, 'learning_rate': 6.184573002754821e-05, 'epoch': 7.63}\n",
      "{'loss': 2.4678, 'learning_rate': 6.170798898071626e-05, 'epoch': 7.66}\n",
      "{'loss': 2.4132, 'learning_rate': 6.15702479338843e-05, 'epoch': 7.69}\n",
      "{'loss': 2.4584, 'learning_rate': 6.143250688705234e-05, 'epoch': 7.71}\n",
      "{'loss': 2.4772, 'learning_rate': 6.12947658402204e-05, 'epoch': 7.74}\n",
      "{'loss': 2.4605, 'learning_rate': 6.115702479338842e-05, 'epoch': 7.77}\n",
      "{'loss': 2.4604, 'learning_rate': 6.1019283746556477e-05, 'epoch': 7.8}\n",
      "{'loss': 2.444, 'learning_rate': 6.0881542699724524e-05, 'epoch': 7.82}\n",
      "{'loss': 2.4317, 'learning_rate': 6.0743801652892564e-05, 'epoch': 7.85}\n",
      "{'loss': 2.4033, 'learning_rate': 6.060606060606061e-05, 'epoch': 7.88}\n",
      "{'loss': 2.3833, 'learning_rate': 6.0468319559228645e-05, 'epoch': 7.91}\n",
      "{'loss': 2.3957, 'learning_rate': 6.03305785123967e-05, 'epoch': 7.93}\n",
      "{'loss': 2.4512, 'learning_rate': 6.0192837465564746e-05, 'epoch': 7.96}\n",
      "{'loss': 2.4342, 'learning_rate': 6.005509641873278e-05, 'epoch': 7.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909f44868b484861afe0873e604fa07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.926367163658142, 'eval_runtime': 96.468, 'eval_samples_per_second': 30.103, 'eval_steps_per_second': 3.763, 'epoch': 8.0}\n",
      "{'loss': 2.2754, 'learning_rate': 5.991735537190083e-05, 'epoch': 8.02}\n",
      "{'loss': 2.3531, 'learning_rate': 5.977961432506888e-05, 'epoch': 8.04}\n",
      "{'loss': 2.4006, 'learning_rate': 5.9641873278236915e-05, 'epoch': 8.07}\n",
      "{'loss': 2.3001, 'learning_rate': 5.950413223140496e-05, 'epoch': 8.1}\n",
      "{'loss': 2.283, 'learning_rate': 5.9366391184573e-05, 'epoch': 8.13}\n",
      "{'loss': 2.3653, 'learning_rate': 5.922865013774105e-05, 'epoch': 8.15}\n",
      "{'loss': 2.2485, 'learning_rate': 5.90909090909091e-05, 'epoch': 8.18}\n",
      "{'loss': 2.2222, 'learning_rate': 5.895316804407714e-05, 'epoch': 8.21}\n",
      "{'loss': 2.4307, 'learning_rate': 5.8815426997245184e-05, 'epoch': 8.24}\n",
      "{'loss': 2.2084, 'learning_rate': 5.867768595041323e-05, 'epoch': 8.26}\n",
      "{'loss': 2.4551, 'learning_rate': 5.8539944903581265e-05, 'epoch': 8.29}\n",
      "{'loss': 2.2734, 'learning_rate': 5.840220385674931e-05, 'epoch': 8.32}\n",
      "{'loss': 2.2684, 'learning_rate': 5.826446280991735e-05, 'epoch': 8.35}\n",
      "{'loss': 2.2494, 'learning_rate': 5.81267217630854e-05, 'epoch': 8.37}\n",
      "{'loss': 2.286, 'learning_rate': 5.798898071625345e-05, 'epoch': 8.4}\n",
      "{'loss': 2.3691, 'learning_rate': 5.785123966942149e-05, 'epoch': 8.43}\n",
      "{'loss': 2.1966, 'learning_rate': 5.7713498622589535e-05, 'epoch': 8.46}\n",
      "{'loss': 2.2391, 'learning_rate': 5.757575757575758e-05, 'epoch': 8.48}\n",
      "{'loss': 2.2854, 'learning_rate': 5.743801652892562e-05, 'epoch': 8.51}\n",
      "{'loss': 2.3341, 'learning_rate': 5.730027548209367e-05, 'epoch': 8.54}\n",
      "{'loss': 2.2818, 'learning_rate': 5.7162534435261704e-05, 'epoch': 8.57}\n",
      "{'loss': 2.327, 'learning_rate': 5.702479338842975e-05, 'epoch': 8.6}\n",
      "{'loss': 2.2868, 'learning_rate': 5.6887052341597805e-05, 'epoch': 8.62}\n",
      "{'loss': 2.2702, 'learning_rate': 5.674931129476584e-05, 'epoch': 8.65}\n",
      "{'loss': 2.218, 'learning_rate': 5.6611570247933886e-05, 'epoch': 8.68}\n",
      "{'loss': 2.1731, 'learning_rate': 5.647382920110193e-05, 'epoch': 8.71}\n",
      "{'loss': 2.2568, 'learning_rate': 5.633608815426997e-05, 'epoch': 8.73}\n",
      "{'loss': 2.3066, 'learning_rate': 5.619834710743802e-05, 'epoch': 8.76}\n",
      "{'loss': 2.2934, 'learning_rate': 5.606060606060606e-05, 'epoch': 8.79}\n",
      "{'loss': 2.2095, 'learning_rate': 5.592286501377411e-05, 'epoch': 8.82}\n",
      "{'loss': 2.1507, 'learning_rate': 5.5785123966942155e-05, 'epoch': 8.84}\n",
      "{'loss': 2.2875, 'learning_rate': 5.564738292011019e-05, 'epoch': 8.87}\n",
      "{'loss': 2.166, 'learning_rate': 5.550964187327824e-05, 'epoch': 8.9}\n",
      "{'loss': 2.2298, 'learning_rate': 5.537190082644629e-05, 'epoch': 8.93}\n",
      "{'loss': 2.2041, 'learning_rate': 5.5234159779614324e-05, 'epoch': 8.95}\n",
      "{'loss': 2.1224, 'learning_rate': 5.509641873278237e-05, 'epoch': 8.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ba62b2a4884a7ab4865ae147fb4989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7794489860534668, 'eval_runtime': 95.9732, 'eval_samples_per_second': 30.258, 'eval_steps_per_second': 3.782, 'epoch': 9.0}\n",
      "{'loss': 2.2123, 'learning_rate': 5.495867768595041e-05, 'epoch': 9.01}\n",
      "{'loss': 2.1514, 'learning_rate': 5.482093663911846e-05, 'epoch': 9.04}\n",
      "{'loss': 2.1921, 'learning_rate': 5.4683195592286506e-05, 'epoch': 9.06}\n",
      "{'loss': 2.2201, 'learning_rate': 5.4545454545454546e-05, 'epoch': 9.09}\n",
      "{'loss': 2.2115, 'learning_rate': 5.4407713498622593e-05, 'epoch': 9.12}\n",
      "{'loss': 2.1104, 'learning_rate': 5.426997245179064e-05, 'epoch': 9.15}\n",
      "{'loss': 2.1099, 'learning_rate': 5.4132231404958674e-05, 'epoch': 9.17}\n",
      "{'loss': 2.1326, 'learning_rate': 5.399449035812673e-05, 'epoch': 9.2}\n",
      "{'loss': 2.0998, 'learning_rate': 5.385674931129476e-05, 'epoch': 9.23}\n",
      "{'loss': 2.1911, 'learning_rate': 5.371900826446281e-05, 'epoch': 9.26}\n",
      "{'loss': 2.1128, 'learning_rate': 5.3581267217630856e-05, 'epoch': 9.28}\n",
      "{'loss': 2.1764, 'learning_rate': 5.34435261707989e-05, 'epoch': 9.31}\n",
      "{'loss': 2.1612, 'learning_rate': 5.3305785123966944e-05, 'epoch': 9.34}\n",
      "{'loss': 2.1947, 'learning_rate': 5.316804407713499e-05, 'epoch': 9.37}\n",
      "{'loss': 2.1569, 'learning_rate': 5.303030303030303e-05, 'epoch': 9.39}\n",
      "{'loss': 2.1331, 'learning_rate': 5.289256198347108e-05, 'epoch': 9.42}\n",
      "{'loss': 2.1246, 'learning_rate': 5.275482093663911e-05, 'epoch': 9.45}\n",
      "{'loss': 2.1565, 'learning_rate': 5.2617079889807166e-05, 'epoch': 9.48}\n",
      "{'loss': 2.0522, 'learning_rate': 5.2479338842975214e-05, 'epoch': 9.5}\n",
      "{'loss': 1.9524, 'learning_rate': 5.234159779614325e-05, 'epoch': 9.53}\n",
      "{'loss': 2.0639, 'learning_rate': 5.2203856749311295e-05, 'epoch': 9.56}\n",
      "{'loss': 2.0891, 'learning_rate': 5.206611570247935e-05, 'epoch': 9.59}\n",
      "{'loss': 2.1135, 'learning_rate': 5.192837465564738e-05, 'epoch': 9.61}\n",
      "{'loss': 2.0771, 'learning_rate': 5.179063360881543e-05, 'epoch': 9.64}\n",
      "{'loss': 2.0876, 'learning_rate': 5.165289256198347e-05, 'epoch': 9.67}\n",
      "{'loss': 2.1339, 'learning_rate': 5.151515151515152e-05, 'epoch': 9.7}\n",
      "{'loss': 2.1586, 'learning_rate': 5.1377410468319564e-05, 'epoch': 9.72}\n",
      "{'loss': 2.1128, 'learning_rate': 5.1239669421487605e-05, 'epoch': 9.75}\n",
      "{'loss': 2.1138, 'learning_rate': 5.110192837465565e-05, 'epoch': 9.78}\n",
      "{'loss': 2.0988, 'learning_rate': 5.09641873278237e-05, 'epoch': 9.81}\n",
      "{'loss': 2.0427, 'learning_rate': 5.082644628099173e-05, 'epoch': 9.83}\n",
      "{'loss': 2.023, 'learning_rate': 5.068870523415979e-05, 'epoch': 9.86}\n",
      "{'loss': 2.1319, 'learning_rate': 5.055096418732782e-05, 'epoch': 9.89}\n",
      "{'loss': 2.0957, 'learning_rate': 5.041322314049587e-05, 'epoch': 9.92}\n",
      "{'loss': 2.0653, 'learning_rate': 5.0275482093663915e-05, 'epoch': 9.94}\n",
      "{'loss': 2.1498, 'learning_rate': 5.0137741046831955e-05, 'epoch': 9.97}\n",
      "{'loss': 2.0625, 'learning_rate': 5e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f7dba48119450d8ca2d21ad1c4f0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6686396598815918, 'eval_runtime': 96.8244, 'eval_samples_per_second': 29.992, 'eval_steps_per_second': 3.749, 'epoch': 10.0}\n",
      "{'loss': 2.0018, 'learning_rate': 4.986225895316804e-05, 'epoch': 10.03}\n",
      "{'loss': 1.9277, 'learning_rate': 4.972451790633609e-05, 'epoch': 10.06}\n",
      "{'loss': 2.0104, 'learning_rate': 4.958677685950414e-05, 'epoch': 10.08}\n",
      "{'loss': 1.969, 'learning_rate': 4.944903581267218e-05, 'epoch': 10.11}\n",
      "{'loss': 2.0687, 'learning_rate': 4.931129476584022e-05, 'epoch': 10.14}\n",
      "{'loss': 1.9572, 'learning_rate': 4.917355371900827e-05, 'epoch': 10.17}\n",
      "{'loss': 1.9963, 'learning_rate': 4.903581267217631e-05, 'epoch': 10.19}\n",
      "{'loss': 1.9722, 'learning_rate': 4.889807162534435e-05, 'epoch': 10.22}\n",
      "{'loss': 2.0101, 'learning_rate': 4.87603305785124e-05, 'epoch': 10.25}\n",
      "{'loss': 1.9424, 'learning_rate': 4.862258953168045e-05, 'epoch': 10.28}\n",
      "{'loss': 1.9768, 'learning_rate': 4.848484848484849e-05, 'epoch': 10.3}\n",
      "{'loss': 1.9815, 'learning_rate': 4.834710743801653e-05, 'epoch': 10.33}\n",
      "{'loss': 1.956, 'learning_rate': 4.8209366391184575e-05, 'epoch': 10.36}\n",
      "{'loss': 2.0799, 'learning_rate': 4.807162534435262e-05, 'epoch': 10.39}\n",
      "{'loss': 2.046, 'learning_rate': 4.793388429752066e-05, 'epoch': 10.41}\n",
      "{'loss': 2.0987, 'learning_rate': 4.779614325068871e-05, 'epoch': 10.44}\n",
      "{'loss': 1.9941, 'learning_rate': 4.765840220385675e-05, 'epoch': 10.47}\n",
      "{'loss': 2.0583, 'learning_rate': 4.75206611570248e-05, 'epoch': 10.5}\n",
      "{'loss': 2.0143, 'learning_rate': 4.738292011019284e-05, 'epoch': 10.52}\n",
      "{'loss': 1.9722, 'learning_rate': 4.7245179063360886e-05, 'epoch': 10.55}\n",
      "{'loss': 1.9988, 'learning_rate': 4.7107438016528926e-05, 'epoch': 10.58}\n",
      "{'loss': 1.9825, 'learning_rate': 4.696969696969697e-05, 'epoch': 10.61}\n",
      "{'loss': 2.0005, 'learning_rate': 4.683195592286502e-05, 'epoch': 10.63}\n",
      "{'loss': 2.0188, 'learning_rate': 4.669421487603306e-05, 'epoch': 10.66}\n",
      "{'loss': 1.9907, 'learning_rate': 4.65564738292011e-05, 'epoch': 10.69}\n",
      "{'loss': 2.0573, 'learning_rate': 4.641873278236915e-05, 'epoch': 10.72}\n",
      "{'loss': 1.9487, 'learning_rate': 4.6280991735537196e-05, 'epoch': 10.74}\n",
      "{'loss': 1.9886, 'learning_rate': 4.6143250688705236e-05, 'epoch': 10.77}\n",
      "{'loss': 1.9263, 'learning_rate': 4.6005509641873277e-05, 'epoch': 10.8}\n",
      "{'loss': 1.9754, 'learning_rate': 4.586776859504133e-05, 'epoch': 10.83}\n",
      "{'loss': 1.9691, 'learning_rate': 4.573002754820937e-05, 'epoch': 10.85}\n",
      "{'loss': 1.9421, 'learning_rate': 4.559228650137741e-05, 'epoch': 10.88}\n",
      "{'loss': 2.0415, 'learning_rate': 4.545454545454546e-05, 'epoch': 10.91}\n",
      "{'loss': 1.9166, 'learning_rate': 4.5316804407713506e-05, 'epoch': 10.94}\n",
      "{'loss': 1.9423, 'learning_rate': 4.5179063360881546e-05, 'epoch': 10.96}\n",
      "{'loss': 1.9583, 'learning_rate': 4.504132231404959e-05, 'epoch': 10.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7408306560e847588f5114ffba119eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5750449895858765, 'eval_runtime': 92.0151, 'eval_samples_per_second': 31.56, 'eval_steps_per_second': 3.945, 'epoch': 11.0}\n",
      "{'loss': 1.9775, 'learning_rate': 4.4903581267217634e-05, 'epoch': 11.02}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1935\u001b[0m ):\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\Programs\\miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\trainer.py:2717\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2715\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m   2716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2717\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32md:\\Programs\\miniconda3\\envs\\llm\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\miniconda3\\envs\\llm\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\miniconda3\\envs\\llm\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text2text_generator = Text2TextGenerationPipeline(model, tokenizer)  \n",
    "# text2text_generator(\"中国的首都是extra0京\", max_length=50, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, {k: v\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m     11\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m     11\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=8, shuffle=True)\n",
    "batch = next(iter(train_loader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)\n",
    "test_loader = DataLoader(tokenized_datasets[\"test\"], batch_size=8, shuffle=False)\n",
    "# 定义损失函数和优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# 定义BLEU评分函数\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference]\n",
    "    candidate = candidate\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "    bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "    bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "    return bleu1, bleu2, bleu3, bleu4\n",
    "\n",
    "# 训练和验证\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "test_bleu1 = []\n",
    "test_bleu2 = []\n",
    "test_bleu3 = []\n",
    "test_bleu4 = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, batch_data in enumerate(train_loader):\n",
    "        print(f\"Batch {batch}: {type(batch_data)}, {batch_data.keys() if isinstance(batch_data, dict) else batch_data}\")\n",
    "        optimizer.zero_grad()\n",
    "        batch_data = batch_data.to(model.device)\n",
    "        ouputs = model(**batch_data)\n",
    "        # input_ids = batch_data[\"input_ids\"].to(model.device)\n",
    "        # attention_mask = batch_data[\"attention_mask\"].to(model.device)\n",
    "        # labels = batch_data[\"labels\"].to(model.device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # 验证\n",
    "    model.eval()\n",
    "    total_bleu1, total_bleu2, total_bleu3, total_bleu4 = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_data in enumerate(test_loader):\n",
    "            input_ids = batch_data[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch_data[\"attention_mask\"].to(model.device)\n",
    "            labels = batch_data[\"labels\"].to(model.device)\n",
    "            \n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=64)\n",
    "            decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            for pred, label in zip(decoded_preds, decoded_labels):\n",
    "                bleu1, bleu2, bleu3, bleu4 = calculate_bleu(label.split(), pred.split())\n",
    "                total_bleu1 += bleu1\n",
    "                total_bleu2 += bleu2\n",
    "                total_bleu3 += bleu3\n",
    "                total_bleu4 += bleu4\n",
    "    \n",
    "    avg_bleu1 = total_bleu1 / len(test_loader.dataset)\n",
    "    avg_bleu2 = total_bleu2 / len(test_loader.dataset)\n",
    "    avg_bleu3 = total_bleu3 / len(test_loader.dataset)\n",
    "    avg_bleu4 = total_bleu4 / len(test_loader.dataset)\n",
    "    \n",
    "    test_bleu1.append(avg_bleu1)\n",
    "    test_bleu2.append(avg_bleu2)\n",
    "    test_bleu3.append(avg_bleu3)\n",
    "    test_bleu4.append(avg_bleu4)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, BLEU-1: {avg_bleu1:.4f}, BLEU-2: {avg_bleu2:.4f}, BLEU-3: {avg_bleu3:.4f}, BLEU-4: {avg_bleu4:.4f}\")\n",
    "\n",
    "# 绘制收敛曲线图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_bleu1, label='BLEU-1')\n",
    "plt.plot(test_bleu2, label='BLEU-2')\n",
    "plt.plot(test_bleu3, label='BLEU-3')\n",
    "plt.plot(test_bleu4, label='BLEU-4')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BLEU Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 预测代码\n",
    "def generate_answer(context, question):\n",
    "    input_text = f\"context:{context} question:{question}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\").input_ids.to(model.device)\n",
    "    output_ids = model.generate(input_ids, max_length=64, do_sample=False)\n",
    "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# 示例预测\n",
    "context = dev_dataset[\"context\"][0]\n",
    "question = dev_dataset[\"question\"][0]\n",
    "print(generate_answer(context, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
