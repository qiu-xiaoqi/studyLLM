{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因果语言模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling,  TrainingArguments, Trainer, BloomForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'completion'],\n",
      "    num_rows: 10000\n",
      "}) \n",
      " {'source': 'wikipedia.zh2307', 'completion': \"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安交通大学的博物馆，馆长是锺明善。\\n历史\\n2004年9月20日开始筹建，2013年4月8日正式建成开馆，位于西安交通大学兴庆校区陕西省西安市咸宁西路28号。建筑面积6,800平米，展厅面积4,500平米，馆藏文物4,900余件。包括历代艺术文物馆、碑石书法馆、西部农民画馆、邢良坤陶瓷艺术馆、陕西秦腔博物馆和书画展厅共五馆一厅。\\n营业时间\\n* 周一至周六：上午九点至十二点，下午一点至五点\\n* 周日闭馆\"}\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(\"../data/wiki_cn_filtered\")\n",
    "print(ds, \"\\n\", ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../Langboat/bloom-389m-zh\")\n",
    "\n",
    "def process_func(examples):\n",
    "    contents = [e + tokenizer.eos_token for e in examples[\"completion\"]] # 添加结束标记\n",
    "    return tokenizer(contents, max_length=384, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409b0e96991e4291b74169725878bb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, batched=True, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# collate_fn用于指定如何将多个样本数据合并成一个batch，mlm指定是否使用掩码语言模型：Masked Language Modeling\n",
    "# mlm=False表示不进行掩码操作，适用于因果语言模型\n",
    "dl = DataLoader(tokenized_ds, batch_size=2, \n",
    "                collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'input_ids': tensor([[    3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3, 13110, 34800,\n",
       "          13535,   916, 33156,    10,   256,   576,   387,   479,   681,  5453,\n",
       "          10955,   915, 24124,  5317, 13110,  6573, 20757, 13535,   355,  5358,\n",
       "           1490,   583, 28056,  1407,  3855,   671,  6113,   189,  6732,  4302,\n",
       "           9488,  3434,  6900,  1322,   355, 37336,  9825,  4608, 13461,  1359,\n",
       "           5358,   355,  5317, 13110, 34800,  4433,  7189, 25722, 29747, 13110,\n",
       "           1498, 12047,  6347, 23563,  2139,  2066,   420, 29288,    25,    15,\n",
       "           7635, 39288,   355,  1484,  5835,  6272,    23,    15,  4180, 39288,\n",
       "            355,  5358,  4516, 11621,    23,    15, 10641,  4887,  1712,   420,\n",
       "           2450, 31163,  8085, 11621,  5358,   553,  9888,  2731, 21335,  5358,\n",
       "            553,  9876, 14011,  4434,  5358,   553, 21484,  4514, 17170, 25871,\n",
       "           8085,  5358,   553, 17489,  6945, 11097, 13535,   641, 33623,  1484,\n",
       "           5835,  1689,  2063,  5358,   569,  5835,   671, 16225,  3422,   189,\n",
       "             13, 23158, 27894, 33227,  1022, 11396,  3347,  1813,  1504,  6566,\n",
       "           1813,   355,  9155,  8633,  1504,  2063,  1813,   189,    13, 23158,\n",
       "            813,  7817,  5358,     2],\n",
       "         [  586, 11835,   739,   355,  8417,  2300,   916, 16892,  5077,   580,\n",
       "          15549,  1434,   996,   307,   387,   355,  1997,  8236,   775,  8417,\n",
       "           2152,  6496, 12176,  3728,  7692,  1503,  1528,  1014, 17887, 18001,\n",
       "           3250,   355,  6250,  3896,  2670, 20882, 16080, 14005,  3993,  1503,\n",
       "          12295,  8930,  3250,   420,  4208,  4326,  5367,  8417,  2152,  4632,\n",
       "          37591,   355,  1379,  8417,  2300, 32824,  3250,  9015, 18945, 16714,\n",
       "           5908, 13551, 30330, 23756,  2152, 15976,   657,  5923,  8417,   586,\n",
       "          16080,  1528,  8941,  5917, 14035,  5895, 14092,  4353, 29445,   355,\n",
       "           8790, 21595,  1450, 15911, 31116, 10345, 29940, 10874,  1125,  4829,\n",
       "          16080,  7093, 22939,   737,   262,   387,    72,   272,   831,   455,\n",
       "             72,   915,  8860, 20725,  1934,  1084,  5478,   420,  4415,  8417,\n",
       "          26263, 12726,   553, 10875, 34820,   355,  1266,  5498,   586, 14907,\n",
       "          32795, 11835,   904, 19934,  1528, 19531, 20517,  1349, 19472, 28879,\n",
       "            671,  8417, 26263, 17020,  5963, 22388, 11900, 12669, 13240,  1042,\n",
       "           9783,   355,  7242,   714,  1806,   775,  6500,   355, 11526, 10185,\n",
       "           1293,  1665, 15984,  7092,  1617,  8417,  2300,   420, 19972, 25622,\n",
       "          10875, 17500, 26523,  2391,  8417,  2300,   355,  6751,  2836, 13539,\n",
       "           8247,   373, 30201,  5498,   420,  8417,  2300,   586,  8523, 19358,\n",
       "           1298, 12176, 30939, 10739,   964,  4318, 10875,   420, 11900, 16080,\n",
       "            904,  9783,   355, 22464,  9658,   355,  8417,  2300, 13561,  2054,\n",
       "           4983,  4829, 30800,  7262,   420, 11394,  8417, 35143, 11937, 15682,\n",
       "           8417,  2300,  7283, 10875,   355,  1016,  4179,  5039, 14027, 26215,\n",
       "          26835,   671, 15095,   189,  1165, 15095, 11900,  6184,  1125,  3244,\n",
       "           3687,   622,  8785,  1121,   891, 13765,   671, 10199,   189,    13,\n",
       "            210,  6940,  3728,  9552,  6082,  8417,  2300,   916,  4375,   714,\n",
       "           3679,  1806, 10567,   915,   189,    13,   210, 16131, 11835,  8417,\n",
       "           2300,   189,    13, 24075,  3728,  8417,  2300,   916,  4829,  3687,\n",
       "           9000, 27689,  8417,  2300,  1300, 11243,  2062, 28431, 27689, 11835,\n",
       "           8417,  2300, 13224,  4829,  3687, 15964,   915,   189,    13, 27340,\n",
       "          11835,  8417,  2300,   189,    13, 27340,  3982,  3728,  8417,  2300,\n",
       "            916,  4375,  2450,  3272,  1234, 19083,   553,  3512,  3121,  1728,\n",
       "            641,  3092,  2113,  7843,   915,   189,    13,   210, 15402,  8417,\n",
       "           2300,   189,    13, 10057,   108, 12693, 14624, 29379,  4719,  6533,\n",
       "            739,   916, 16148, 10981, 21350,  9067,  1203,  8931,  1258, 11835,\n",
       "           4719,  6533,   739, 20393,   189,    13,   210, 19546,  1517, 11835,\n",
       "           8417,  2300,   189,    13,   210, 23928,   168,   117,   245,  6279,\n",
       "            114,   240,   170,   100,   124,   168,   117,   228,  6279,   100,\n",
       "            124,   168,   117,   228,   171,   238,   224, 41356,   236, 24175,\n",
       "          11082, 10981, 21350,  9067]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 13110, 34800,\n",
       "          13535,   916, 33156,    10,   256,   576,   387,   479,   681,  5453,\n",
       "          10955,   915, 24124,  5317, 13110,  6573, 20757, 13535,   355,  5358,\n",
       "           1490,   583, 28056,  1407,  3855,   671,  6113,   189,  6732,  4302,\n",
       "           9488,  3434,  6900,  1322,   355, 37336,  9825,  4608, 13461,  1359,\n",
       "           5358,   355,  5317, 13110, 34800,  4433,  7189, 25722, 29747, 13110,\n",
       "           1498, 12047,  6347, 23563,  2139,  2066,   420, 29288,    25,    15,\n",
       "           7635, 39288,   355,  1484,  5835,  6272,    23,    15,  4180, 39288,\n",
       "            355,  5358,  4516, 11621,    23,    15, 10641,  4887,  1712,   420,\n",
       "           2450, 31163,  8085, 11621,  5358,   553,  9888,  2731, 21335,  5358,\n",
       "            553,  9876, 14011,  4434,  5358,   553, 21484,  4514, 17170, 25871,\n",
       "           8085,  5358,   553, 17489,  6945, 11097, 13535,   641, 33623,  1484,\n",
       "           5835,  1689,  2063,  5358,   569,  5835,   671, 16225,  3422,   189,\n",
       "             13, 23158, 27894, 33227,  1022, 11396,  3347,  1813,  1504,  6566,\n",
       "           1813,   355,  9155,  8633,  1504,  2063,  1813,   189,    13, 23158,\n",
       "            813,  7817,  5358,     2],\n",
       "         [  586, 11835,   739,   355,  8417,  2300,   916, 16892,  5077,   580,\n",
       "          15549,  1434,   996,   307,   387,   355,  1997,  8236,   775,  8417,\n",
       "           2152,  6496, 12176,  3728,  7692,  1503,  1528,  1014, 17887, 18001,\n",
       "           3250,   355,  6250,  3896,  2670, 20882, 16080, 14005,  3993,  1503,\n",
       "          12295,  8930,  3250,   420,  4208,  4326,  5367,  8417,  2152,  4632,\n",
       "          37591,   355,  1379,  8417,  2300, 32824,  3250,  9015, 18945, 16714,\n",
       "           5908, 13551, 30330, 23756,  2152, 15976,   657,  5923,  8417,   586,\n",
       "          16080,  1528,  8941,  5917, 14035,  5895, 14092,  4353, 29445,   355,\n",
       "           8790, 21595,  1450, 15911, 31116, 10345, 29940, 10874,  1125,  4829,\n",
       "          16080,  7093, 22939,   737,   262,   387,    72,   272,   831,   455,\n",
       "             72,   915,  8860, 20725,  1934,  1084,  5478,   420,  4415,  8417,\n",
       "          26263, 12726,   553, 10875, 34820,   355,  1266,  5498,   586, 14907,\n",
       "          32795, 11835,   904, 19934,  1528, 19531, 20517,  1349, 19472, 28879,\n",
       "            671,  8417, 26263, 17020,  5963, 22388, 11900, 12669, 13240,  1042,\n",
       "           9783,   355,  7242,   714,  1806,   775,  6500,   355, 11526, 10185,\n",
       "           1293,  1665, 15984,  7092,  1617,  8417,  2300,   420, 19972, 25622,\n",
       "          10875, 17500, 26523,  2391,  8417,  2300,   355,  6751,  2836, 13539,\n",
       "           8247,   373, 30201,  5498,   420,  8417,  2300,   586,  8523, 19358,\n",
       "           1298, 12176, 30939, 10739,   964,  4318, 10875,   420, 11900, 16080,\n",
       "            904,  9783,   355, 22464,  9658,   355,  8417,  2300, 13561,  2054,\n",
       "           4983,  4829, 30800,  7262,   420, 11394,  8417, 35143, 11937, 15682,\n",
       "           8417,  2300,  7283, 10875,   355,  1016,  4179,  5039, 14027, 26215,\n",
       "          26835,   671, 15095,   189,  1165, 15095, 11900,  6184,  1125,  3244,\n",
       "           3687,   622,  8785,  1121,   891, 13765,   671, 10199,   189,    13,\n",
       "            210,  6940,  3728,  9552,  6082,  8417,  2300,   916,  4375,   714,\n",
       "           3679,  1806, 10567,   915,   189,    13,   210, 16131, 11835,  8417,\n",
       "           2300,   189,    13, 24075,  3728,  8417,  2300,   916,  4829,  3687,\n",
       "           9000, 27689,  8417,  2300,  1300, 11243,  2062, 28431, 27689, 11835,\n",
       "           8417,  2300, 13224,  4829,  3687, 15964,   915,   189,    13, 27340,\n",
       "          11835,  8417,  2300,   189,    13, 27340,  3982,  3728,  8417,  2300,\n",
       "            916,  4375,  2450,  3272,  1234, 19083,   553,  3512,  3121,  1728,\n",
       "            641,  3092,  2113,  7843,   915,   189,    13,   210, 15402,  8417,\n",
       "           2300,   189,    13, 10057,   108, 12693, 14624, 29379,  4719,  6533,\n",
       "            739,   916, 16148, 10981, 21350,  9067,  1203,  8931,  1258, 11835,\n",
       "           4719,  6533,   739, 20393,   189,    13,   210, 19546,  1517, 11835,\n",
       "           8417,  2300,   189,    13,   210, 23928,   168,   117,   245,  6279,\n",
       "            114,   240,   170,   100,   124,   168,   117,   228,  6279,   100,\n",
       "            124,   168,   117,   228,   171,   238,   224, 41356,   236, 24175,\n",
       "          11082, 10981, 21350,  9067]])})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enumerate:将对象转换为一个枚举对象并返回：（index，batch）元组，index是batch的索引\n",
    "# next:返回枚举对象的下一个值，第一次调用表示反返回第一个批次的数据\n",
    "next(enumerate(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"../Langboat/bloom-389m-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16, # 梯度累加，结合per_device_train_batch_size=2，表示累加32个batch\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True, # fp16精度训练，加速训练\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\柒\\AppData\\Local\\Temp\\ipykernel_32732\\3774039206.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "d:\\miniconda3\\envs\\llm\\lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664d598f805544f192c1d45f6ab009bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9978, 'grad_norm': 706.550537109375, 'learning_rate': 4.3910256410256415e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\trainer.py:2483\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m-> 2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安市大唐芙蓉园，西安铁路铁路枢纽西安市火车站的综合性博物馆。馆藏有中国古代珍贵玉器和珍贵雕像。展藏面积达三十多吨。为国内唯一一座拥有国内重点文物保护单位之博物馆。馆藏玉器和雕像均以珍贵方式收藏，其中出土的一尊秦始皇大型雕像，出土的秦始皇小型雕像，出土的一尊中国古朴、朴实的名人雕像等珍贵雕像是展馆的精品。\\n该展馆为二层，五楼相连，楼檐下为歇山式檐\"}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安\", max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常迅猛，许多的游戏玩家们在游戏里得到了一些收益，也得到了他们想要的东西，比如，他们想拥有的是比他们自己的游戏更高的利润。由于《尤达求生存》、《全境封锁》、《堡垒之夜》、《战锤2》、《战争》、《暴躁怪物》、《暴躁怪物2》、《生化危机5》、《生化危机》（生化危机 5是所有系列第一部的续集）是第一款由索尼和TNT合作制作的动作类SAP游戏，而《战争》、《暴'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常\", max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
