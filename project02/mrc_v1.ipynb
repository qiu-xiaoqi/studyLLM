{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于滑动窗口截断的机器阅读理解任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict \n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'TRAIN_186_QUERY_0',\n",
       " 'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       " 'question': '范廷颂是什么时候被任为主教的？',\n",
       " 'answers': {'text': ['1963年'], 'answer_start': [30]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = DatasetDict.load_from_disk(\"../data/mrc_data_copy\")\n",
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3 数据预处理\n",
    "- 先加载tokenizer\n",
    "- 查看数据集的结构,其键features的结构为：['id', 'context', 'question', 'answers']\n",
    "- 然后对数据进行tokenizer处理，得到tokenizer处理后的数据集，其键为：['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping']，其中'offset_mapping'为每个token在原始句子中的位置，'overflow_to_sample_mapping'为每个token对应的句子的索引,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='../hfl/chinese-macbert-base', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../hfl/chinese-macbert-base\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'context', 'question', 'answers'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "sample_dataset = datasets[\"train\"].select(range(10))\n",
    "print(sample_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对sample_dataset进行tokenizer处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_examples = tokenizer(text=sample_dataset[\"question\"],\n",
    "                               text_pair=sample_dataset[\"context\"],\n",
    "                               return_offsets_mapping=True,\n",
    "                               return_overflowing_tokens=True,\n",
    "                               stride=128,\n",
    "                               max_length=384,\n",
    "                               truncation=\"only_second\",\n",
    "                               padding=\"max_length\")\n",
    "tokenized_examples.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现当tokenizer加入return_overflowing_tokens=True时，overflow_to_sample_mapping的索引值会变大，所以需要用overflow_to_sample_mapping[i]来获取对应的sample_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9],\n",
       " 29)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_examples[\"overflow_to_sample_mapping\"], len(tokenized_examples[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对前三个示例进行decode可以发现，前三段的问题都是一样的，但是内容是相连的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生 ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 [SEP]\n",
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 的 爱 」 。 由 于 范 廷 颂 被 越 南 政 府 软 禁 差 不 多 30 年 ， 因 此 他 无 法 到 所 属 堂 区 进 行 牧 灵 工 作 而 专 注 研 读 等 工 作 。 范 廷 颂 除 了 面 对 战 争 、 贫 困 、 被 当 局 迫 害 天 主 教 会 等 问 题 外 ， 也 秘 密 恢 复 修 院 、 创 建 女 修 会 团 体 等 。 1990 年 ， 教 宗 若 望 保 禄 二 世 在 同 年 6 月 18 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 [SEP]\n",
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 月 26 日 ， 教 宗 若 望 保 禄 二 世 任 命 天 主 教 谅 山 教 区 兼 天 主 教 高 平 教 区 吴 光 杰 主 教 为 天 主 教 河 内 总 教 区 署 理 主 教 ； 及 至 2005 年 2 月 19 日 ， 范 廷 颂 因 获 批 辞 去 总 主 教 职 务 而 荣 休 ； 吴 光 杰 同 日 真 除 天 主 教 河 内 总 教 区 总 主 教 职 务 。 范 廷 颂 于 2009 年 2 月 22 日 清 晨 在 河 内 离 世 ， 享 年 89 岁 ； 其 葬 礼 于 同 月 26 日 上 午 在 天 主 教 河 内 总 教 区 总 主 教 座 堂 举 行 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for sen in tokenizer.batch_decode(tokenized_examples[\"input_ids\"][:3]):\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "print(sample_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 0\n",
      "2 0\n",
      "3 1\n",
      "4 1\n",
      "5 1\n",
      "6 2\n",
      "7 2\n",
      "8 2\n",
      "9 3\n",
      "10 3\n",
      "11 3\n",
      "12 4\n",
      "13 4\n",
      "14 4\n",
      "15 5\n",
      "16 5\n",
      "17 5\n",
      "18 6\n",
      "19 6\n",
      "20 6\n",
      "21 7\n",
      "22 7\n",
      "23 7\n",
      "24 8\n",
      "25 8\n",
      "26 8\n",
      "27 9\n",
      "28 9\n"
     ]
    }
   ],
   "source": [
    "for idx, _ in enumerate(sample_mapping):\n",
    "    print(idx, sample_mapping[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['1963年'], 'answer_start': [30]} 30 35 17 382 47 48 [[0]]\n",
      "token answer decode: 1963 年\n",
      "{'text': ['1963年'], 'answer_start': [30]} 30 35 17 382 0 48 []\n",
      "token answer decode: [CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但\n",
      "{'text': ['1963年'], 'answer_start': [30]} 30 35 17 289 0 48 []\n",
      "token answer decode: [CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。\n",
      "{'text': ['1990年被擢升为天主教河内总教区宗座署理'], 'answer_start': [41]} 41 62 15 382 53 70 [[1]]\n",
      "token answer decode: 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理\n",
      "{'text': ['1990年被擢升为天主教河内总教区宗座署理'], 'answer_start': [41]} 41 62 15 382 0 70 []\n",
      "token answer decode: [CLS] 1990 年 ， 范 廷 颂 担 任 什 么 职 务 ？ [SEP] 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因\n",
      "{'text': ['1990年被擢升为天主教河内总教区宗座署理'], 'answer_start': [41]} 41 62 15 283 0 70 []\n",
      "token answer decode: [CLS] 1990 年 ， 范 廷 颂 担 任 什 么 职 务 ？ [SEP] 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总\n",
      "{'text': ['范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'], 'answer_start': [97]} 97 126 15 382 100 124 [[2]]\n",
      "token answer decode: 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生\n",
      "{'text': ['范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'], 'answer_start': [97]} 97 126 15 382 0 124 []\n",
      "token answer decode: [CLS] 范 廷 颂 是 于 何 时 何 地 出 生 的 ？ [SEP] 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ，\n",
      "{'text': ['范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'], 'answer_start': [97]} 97 126 15 283 0 124 []\n",
      "token answer decode: [CLS] 范 廷 颂 是 于 何 时 何 地 出 生 的 ？ [SEP] 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任\n",
      "{'text': ['1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理'], 'answer_start': [548]} 548 598 17 382 0 124 []\n",
      "token answer decode: [CLS] 1994 年 3 月 ， 范 廷 颂 担 任 什 么 职 务 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区\n",
      "{'text': ['1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理'], 'answer_start': [548]} 548 598 17 382 287 332 [[3]]\n",
      "token answer decode: 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理\n",
      "{'text': ['1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理'], 'answer_start': [548]} 548 598 17 289 49 94 [[3]]\n",
      "token answer decode: 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理\n",
      "{'text': ['范廷颂于2009年2月22日清晨在河内离世'], 'answer_start': [759]} 759 780 12 382 0 94 []\n",
      "token answer decode: [CLS] 范 廷 颂 是 何 时 去 世 的 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离\n",
      "{'text': ['范廷颂于2009年2月22日清晨在河内离世'], 'answer_start': [759]} 759 780 12 382 0 94 []\n",
      "token answer decode: [CLS] 范 廷 颂 是 何 时 去 世 的 ？ [SEP] 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963\n",
      "{'text': ['范廷颂于2009年2月22日清晨在河内离世'], 'answer_start': [759]} 759 780 12 274 225 241 [[4]]\n",
      "token answer decode: 范 廷 颂 于 2009 年 2 月 22 日 清 晨 在 河 内 离 世\n",
      "{'text': ['《全美超级模特儿新秀大赛》第十季'], 'answer_start': [26]} 26 42 21 382 47 62 [[5]]\n",
      "token answer decode: 《 全 美 超 级 模 特 儿 新 秀 大 赛 》 第 十 季\n",
      "{'text': ['《全美超级模特儿新秀大赛》第十季'], 'answer_start': [26]} 26 42 21 382 0 62 []\n",
      "token answer decode: [CLS] 安 雅 · 罗 素 法 参 加 了 什 么 比 赛 获 得 了 亚 军 ？ [SEP] 模 特 儿 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入\n",
      "{'text': ['《全美超级模特儿新秀大赛》第十季'], 'answer_start': [26]} 26 42 21 174 0 62 []\n",
      "token answer decode: [CLS] 安 雅 · 罗 素 法 参 加 了 什 么 比 赛 获 得 了 亚 军 ？ [SEP] 第 一 ， 也 胜 出 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志\n",
      "{'text': ['有前途的新面孔'], 'answer_start': [247]} 247 254 20 382 232 238 [[6]]\n",
      "token answer decode: 有 前 途 的 新 面 孔\n",
      "{'text': ['有前途的新面孔'], 'answer_start': [247]} 247 254 20 382 0 238 []\n",
      "token answer decode: [CLS] russell tanoue 对 安 雅 · 罗 素 法 的 评 价 是 什 么 ？ [SEP] 特 儿 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 计 的 晚 装 。 在 最 后 两 强 中 ， 安 雅 与 另 一 名 参 赛 者 惠 妮 · 汤 姆 森 为 范 思 哲 走 秀 ， 但 评 判 认 为 她 在 台 上 不 够 惠 妮 突 出 ， 所 以 选 了 惠 妮 当 冠 军 ， 安 雅 屈 居 亚 军 ( 但 就 整 体 表 现 来 说 ， 部 份 网 友 认 为 安 雅 才 是 第 十 季 名 副 其\n",
      "{'text': ['有前途的新面孔'], 'answer_start': [247]} 247 254 20 171 0 238 []\n",
      "token answer decode: [CLS] russell tanoue 对 安 雅 · 罗 素 法 的 评 价 是 什 么 ？ [SEP] ， 也 胜 出 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志 拍 摄 封 面 及 内 页 照 。 其 后 她 参 加 了 v 杂 志 与 supreme 模 特 儿 公 司 合 办 的 模 特 儿 选 拔 赛 2008 。 她 其 后 更 与 elite 签 约 。 最 近 她 与 香 港 的 模 特 儿 公 司 style international management 签 约 ， 并 在 香 港 发 展 其 模 特 儿 事 业 。 她 曾 在 很 多 香 港 的 时 装 杂 志 中 任 模 特 儿 ， 《 jet 》 、 《 东 方 日 报 》 、 《 elle 》 等 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "{'text': ['《Jet》、《东方日报》、《Elle》等'], 'answer_start': [706]} 706 726 20 382 0 238 []\n",
      "token answer decode: [CLS] 安 雅 · 罗 素 法 合 作 过 的 香 港 杂 志 有 哪 些 ？ [SEP] 安 雅 · 罗 素 法 （ ， ） ， 来 自 俄 罗 斯 圣 彼 得 堡 的 模 特 儿 。 她 是 《 全 美 超 级 模 特 儿 新 秀 大 赛 》 第 十 季 的 亚 军 。 2008 年 ， 安 雅 宣 布 改 回 出 生 时 的 名 字 ： 安 雅 · 罗 素 法 （ anya rozova ） ， 在 此 之 前 是 使 用 安 雅 · 冈 （ ） 。 安 雅 于 俄 罗 斯 出 生 ， 后 来 被 一 个 居 住 在 美 国 夏 威 夷 群 岛 欧 胡 岛 檀 香 山 的 家 庭 领 养 。 安 雅 十 七 岁 时 曾 参 与 香 奈 儿 、 路 易 · 威 登 及 芬 迪 （ fendi ） 等 品 牌 的 非 正 式 时 装 秀 。 2007 年 ， 她 于 瓦 伊 帕 胡 高 级 中 学 毕 业 。 毕 业 后 ， 她 当 了 一 名 售 货 员 。 她 曾 为 russell tanoue 拍 摄 照 片 ， russell tanoue 称 赞 她 是 「 有 前 途 的 新 面 孔\n",
      "{'text': ['《Jet》、《东方日报》、《Elle》等'], 'answer_start': [706]} 706 726 20 382 0 238 []\n",
      "token answer decode: [CLS] 安 雅 · 罗 素 法 合 作 过 的 香 港 杂 志 有 哪 些 ？ [SEP] 特 儿 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 计 的 晚 装 。 在 最 后 两 强 中 ， 安 雅 与 另 一 名 参 赛 者 惠 妮 · 汤 姆 森 为 范 思 哲 走 秀 ， 但 评 判 认 为 她 在 台 上 不 够 惠 妮 突 出 ， 所 以 选 了 惠 妮 当 冠 军 ， 安 雅 屈 居 亚 军 ( 但 就 整 体 表 现 来 说 ， 部 份 网 友 认 为 安 雅 才 是 第 十 季 名 副 其\n",
      "{'text': ['《Jet》、《东方日报》、《Elle》等'], 'answer_start': [706]} 706 726 20 171 155 170 [[7]]\n",
      "token answer decode: 《 jet 》 、 《 东 方 日 报 》 、 《 elle 》 等\n",
      "{'text': ['售货员'], 'answer_start': [202]} 202 205 18 382 205 207 [[8]]\n",
      "token answer decode: 售 货 员\n",
      "{'text': ['售货员'], 'answer_start': [202]} 202 205 18 382 0 207 []\n",
      "token answer decode: [CLS] 毕 业 后 的 安 雅 · 罗 素 法 职 业 是 什 么 ？ [SEP] 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 计 的 晚 装 。 在 最 后 两 强 中 ， 安 雅 与 另 一 名 参 赛 者 惠 妮 · 汤 姆 森 为 范 思 哲 走 秀 ， 但 评 判 认 为 她 在 台 上 不 够 惠 妮 突 出 ， 所 以 选 了 惠 妮 当 冠 军 ， 安 雅 屈 居 亚\n",
      "{'text': ['售货员'], 'answer_start': [202]} 202 205 18 165 0 207 []\n",
      "token answer decode: [CLS] 毕 业 后 的 安 雅 · 罗 素 法 职 业 是 什 么 ？ [SEP] 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志 拍 摄 封 面 及 内 页 照 。 其 后 她 参 加 了 v 杂 志 与 supreme 模 特 儿 公 司 合 办 的 模 特 儿 选 拔 赛 2008 。 她 其 后 更 与 elite 签 约 。 最 近 她 与 香 港 的 模 特 儿 公 司 style international management 签 约 ， 并 在 香 港 发 展 其 模 特 儿 事 业 。 她 曾 在 很 多 香 港 的 时 装 杂 志 中 任 模 特 儿 ， 《 jet 》 、 《 东 方 日 报 》 、 《 elle 》 等 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "{'text': ['大空翼'], 'answer_start': [84]} 84 87 21 382 105 107 [[9]]\n",
      "token answer decode: 大 空 翼\n",
      "{'text': ['大空翼'], 'answer_start': [84]} 84 87 21 252 0 107 []\n",
      "token answer decode: [CLS] 岬 太 郎 在 第 一 次 南 葛 市 生 活 时 的 搭 档 是 谁 ？ [SEP] 衣 号 码 ： 11 担 任 位 置 ： 中 场 、 攻 击 中 场 、 右 中 场 擅 长 脚 ： 右 脚 所 属 队 伍 ： 盘 田 山 叶 故 事 发 展 岬 太 郎 在 小 学 期 间 不 断 转 换 学 校 ， 在 南 葛 小 学 就 读 时 在 全 国 大 赛 中 夺 得 冠 军 ； 国 中 三 年 随 父 亲 孤 单 地 在 法 国\n"
     ]
    }
   ],
   "source": [
    "for idx, _ in enumerate(sample_mapping):\n",
    "    answer = sample_dataset[\"answers\"][sample_mapping[idx]]\n",
    "    star_char = answer[\"answer_start\"][0]\n",
    "    end_char = star_char + len(answer[\"text\"][0]) # 左闭右开，不需要减1\n",
    "\n",
    "    context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "    context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1 # None 位置的上一个位置为结束为 \n",
    "    \n",
    "    offset = tokenized_examples.get(\"offset_mapping\")[idx]\n",
    "    example_ids = []\n",
    "    # 判断答案是否在context中\n",
    "    if offset[context_end][1] < star_char or offset[context_start][0] > end_char:\n",
    "        start_token_pos = 0\n",
    "        end__token_pos = 0\n",
    "    else:\n",
    "        tokenid = context_start\n",
    "        while tokenid <= context_end and offset[tokenid][0] < star_char:\n",
    "            tokenid += 1\n",
    "        start_token_pos = tokenid\n",
    "        tokenid = context_end\n",
    "        while tokenid >= context_start and offset[tokenid][1] > end_char:\n",
    "            tokenid -= 1\n",
    "        end_token_pos = tokenid\n",
    "        example_ids.append([sample_mapping[idx]])\n",
    "    \n",
    "    print(answer, star_char, end_char, context_start, context_end, start_token_pos, end_token_pos, example_ids)\n",
    "    print(\"token answer decode:\", tokenizer.decode(tokenized_examples[\"input_ids\"][idx][start_token_pos: end_token_pos + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(examples):\n",
    "    tokenized_examples = tokenizer(text=examples[\"question\"],\n",
    "                               text_pair=examples[\"context\"],\n",
    "                               return_offsets_mapping=True,\n",
    "                               return_overflowing_tokens=True,\n",
    "                               stride=128,\n",
    "                               max_length=384,\n",
    "                               truncation=\"only_second\",\n",
    "                               padding=\"max_length\")\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    start_positions= []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for idx, _ in enumerate(sample_mapping):\n",
    "        answer = examples[\"answers\"][sample_mapping[idx]]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "        context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "        offset = tokenized_examples.get(\"offset_mapping\")[idx]\n",
    "\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            tokenid = context_start\n",
    "            while tokenid <= context_end and offset[tokenid][0] < start_char:\n",
    "                tokenid += 1\n",
    "            start_token_pos = tokenid\n",
    "            tokenid = context_end\n",
    "            while tokenid >= context_start and offset[tokenid][1] > end_char:\n",
    "                tokenid -= 1\n",
    "            end_token_pos = tokenid\n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "        example_ids.append(examples[\"id\"][sample_mapping[idx]])\n",
    "        # 把问题变成None\n",
    "        tokenized_examples[\"offset_mapping\"][idx] = [\n",
    "            (o if tokenized_examples.sequence_ids(idx)[k] == 1 else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][idx])\n",
    "        ]\n",
    "\n",
    "    tokenized_examples[\"example_ids\"] = example_ids\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    return tokenized_examples   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_ids', 'start_positions', 'end_positions'],\n",
       "        num_rows: 19189\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_ids', 'start_positions', 'end_positions'],\n",
       "        num_rows: 6327\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_ids', 'start_positions', 'end_positions'],\n",
       "        num_rows: 1988\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现offset_mapping的query全为None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, [266, 267], [267, 268], [268, 269], [269, 270], [270, 271], [271, 272], [272, 273], [273, 274], [274, 275], [275, 276], [276, 277], [277, 278], [278, 279], [279, 280], [280, 281], [281, 282], [282, 283], [283, 284], [284, 285], [285, 286], [286, 287], [287, 288], [288, 289], [289, 290], [290, 291], [291, 292], [292, 293], [293, 294], [294, 295], [295, 296], [296, 297], [297, 298], [298, 299], [299, 300], [300, 301], [301, 302], [302, 303], [303, 304], [304, 305], [305, 306], [306, 307], [307, 308], [308, 309], [309, 310], [310, 311], [311, 312], [312, 313], [313, 314], [314, 315], [315, 316], [316, 317], [317, 318], [318, 319], [319, 320], [320, 321], [321, 325], [325, 326], [326, 327], [327, 328], [328, 329], [329, 330], [330, 331], [331, 332], [332, 333], [333, 334], [334, 335], [335, 336], [336, 337], [337, 338], [338, 339], [339, 340], [340, 341], [341, 342], [342, 343], [343, 344], [344, 345], [345, 346], [346, 347], [347, 348], [348, 349], [349, 350], [350, 351], [351, 352], [352, 353], [353, 354], [354, 355], [355, 356], [356, 360], [360, 361], [361, 362], [362, 363], [363, 364], [364, 365], [365, 366], [366, 367], [367, 368], [368, 369], [369, 370], [370, 371], [371, 372], [372, 373], [373, 374], [374, 375], [375, 376], [376, 377], [377, 378], [378, 379], [379, 380], [380, 381], [381, 382], [382, 383], [383, 384], [384, 385], [385, 386], [386, 387], [387, 388], [388, 390], [390, 391], [391, 392], [392, 393], [393, 394], [394, 395], [395, 396], [396, 397], [397, 398], [398, 399], [399, 400], [400, 401], [401, 402], [402, 403], [403, 404], [404, 405], [405, 406], [406, 407], [407, 408], [408, 409], [409, 410], [410, 411], [411, 412], [412, 413], [413, 414], [414, 415], [415, 416], [416, 417], [417, 418], [418, 419], [419, 420], [420, 421], [421, 422], [422, 424], [424, 425], [425, 426], [426, 427], [427, 428], [428, 429], [429, 430], [430, 431], [431, 432], [432, 433], [433, 434], [434, 435], [435, 436], [436, 437], [437, 438], [438, 439], [439, 440], [440, 441], [441, 442], [442, 443], [443, 444], [444, 445], [445, 446], [446, 447], [447, 448], [448, 449], [449, 450], [450, 451], [451, 452], [452, 453], [453, 454], [454, 455], [455, 456], [456, 457], [457, 458], [458, 459], [459, 460], [460, 461], [461, 462], [462, 463], [463, 464], [464, 465], [465, 466], [466, 467], [467, 468], [468, 469], [469, 470], [470, 471], [471, 472], [472, 473], [473, 474], [474, 475], [475, 476], [476, 477], [477, 478], [478, 479], [479, 480], [480, 481], [481, 482], [482, 483], [483, 484], [484, 485], [485, 486], [486, 487], [487, 488], [488, 489], [489, 490], [490, 491], [491, 492], [492, 493], [493, 494], [494, 495], [495, 499], [499, 500], [500, 501], [501, 502], [502, 503], [503, 504], [504, 505], [505, 506], [506, 507], [507, 508], [508, 509], [509, 510], [510, 511], [511, 512], [512, 513], [513, 514], [514, 516], [516, 517], [517, 518], [518, 519], [519, 520], [520, 521], [521, 522], [522, 523], [523, 524], [524, 525], [525, 526], [526, 527], [527, 528], [528, 529], [529, 530], [530, 531], [531, 532], [532, 533], [533, 534], [534, 535], [535, 536], [536, 537], [537, 538], [538, 539], [539, 540], [540, 541], [541, 542], [542, 543], [543, 544], [544, 545], [545, 546], [546, 547], [547, 548], [548, 552], [552, 553], [553, 554], [554, 555], [555, 557], [557, 558], [558, 559], [559, 560], [560, 561], [561, 562], [562, 563], [563, 564], [564, 565], [565, 566], [566, 567], [567, 568], [568, 569], [569, 570], [570, 571], [571, 572], [572, 573], [573, 574], [574, 575], [575, 576], [576, 577], [577, 578], [578, 579], [579, 580], [580, 581], [581, 582], [582, 583], [583, 584], [584, 585], [585, 586], [586, 587], [587, 588], [588, 589], [589, 590], [590, 591], [591, 592], [592, 593], [593, 594], [594, 595], [595, 596], [596, 597], [597, 598], [598, 599], [599, 600], [600, 601], [601, 603], [603, 604], [604, 606], [606, 607], [607, 608], [608, 609], [609, 610], [610, 611], [611, 612], [612, 613], [613, 614], [614, 615], [615, 616], [616, 617], [617, 618], [618, 619], [619, 620], [620, 621], [621, 622], [622, 623], [623, 624], [624, 625], [625, 626], [626, 627], [627, 631], [631, 632], [632, 633], [633, 637], [637, 638], [638, 639], [639, 640], [640, 641], [641, 642], [642, 643], [643, 644], [644, 645], [645, 646], [646, 647], [647, 648], [648, 649], [649, 650], [650, 651], [651, 652], [652, 653], [653, 657], [657, 658], [658, 659], None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][\"offset_mapping\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TRAIN_186_QUERY_0', 'TRAIN_186_QUERY_0', 'TRAIN_186_QUERY_0', 'TRAIN_186_QUERY_1', 'TRAIN_186_QUERY_1', 'TRAIN_186_QUERY_1', 'TRAIN_186_QUERY_2', 'TRAIN_186_QUERY_2', 'TRAIN_186_QUERY_2', 'TRAIN_186_QUERY_3']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][\"example_ids\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example和feature有一个映射关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'TRAIN_186_QUERY_0': [0, 1, 2],\n",
       "             'TRAIN_186_QUERY_1': [3, 4, 5],\n",
       "             'TRAIN_186_QUERY_2': [6, 7, 8],\n",
       "             'TRAIN_186_QUERY_3': [9]})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "example_to_feature = collections.defaultdict(list)\n",
    "for idx, example_id in enumerate(tokenized_datasets[\"train\"][\"example_ids\"][:10]):\n",
    "    example_to_feature[example_id].append(idx)\n",
    "\n",
    "example_to_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4 获取模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def get_result(start_logits, end_logits, examples, features):\n",
    "    \"\"\"\n",
    "    examples: 原datasets\n",
    "    features: tokenized后的datasets\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = {} # 预测值\n",
    "    references = {} # 真实值\n",
    "\n",
    "    # example 和 feature 的映射\n",
    "    example_to_feature = collections.defaultdict(list)\n",
    "    for idx, example_id in enumerate(features[\"example_ids\"]):\n",
    "        example_to_feature[example_id].append(idx)\n",
    "\n",
    "    # 最优答案候选\n",
    "    n_best = 20\n",
    "    # 最大答案长度\n",
    "    max_answer_length = 30\n",
    "\n",
    "    # 获取原数据的文本和id，因为tokenized后的features：features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_ids', 'start_positions', 'end_positions'],\n",
    "    for example in examples:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        # 根据example和feature的映射关系获取start_logit，end_logit\n",
    "        for feature_idx in example_to_feature[example_id]:\n",
    "            start_logit = start_logits[feature_idx]\n",
    "            end_logit = end_logits[feature_idx]\n",
    "            offset = features[feature_idx][\"offset_mapping\"]\n",
    "            start_indexs = np.argsort(start_logit)[::-1][:n_best].tolist()\n",
    "            end_indexs = np.argsort(end_logit)[::-1][:n_best].tolist()\n",
    "\n",
    "            for start_index in start_indexs:\n",
    "                for end_index in end_indexs:\n",
    "                    if offset[start_index] is None or offset[end_index] is None:\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    answers.append({\n",
    "                        \"text\": context[offset[start_index][0]: offset[end_index][1]],\n",
    "                        \"score\": start_logit[start_index] + end_logit[end_index]\n",
    "                    })\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"score\"])\n",
    "            predictions[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predictions[example_id] = \"\"\n",
    "        references[example_id] = example[\"answers\"][\"text\"]\n",
    "\n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmrc_eval import evaluate_cmrc\n",
    "\n",
    "def metirc(pred):\n",
    "    start_logits, end_logits = pred[0]\n",
    "    if start_logits.shape[0] == len(tokenized_datasets[\"validation\"]):\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"validation\"], tokenized_datasets[\"validation\"])\n",
    "    else:\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"test\"], tokenized_datasets[\"test\"])\n",
    "    return evaluate_cmrc(p,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at ../hfl/chinese-macbert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../hfl/chinese-macbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"../hfl/chinese-macbert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step7 配置TraningArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir=\"v1_result\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step8 配置Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=metirc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step9 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400016d77c294b48977cb817412a4a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
